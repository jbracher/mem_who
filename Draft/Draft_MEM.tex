\documentclass[12pt]{article}
\usepackage[left=1.5cm,right=1.5cm, top = 2cm, bottom = 2cm]{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{todonotes}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{amsmath}

\newcommand{\sd}{s}
\newcommand{\mean}{\bar{y}}

\usepackage{caption}
\captionsetup[table]{name={{\footnotesize Table}}}
\usepackage{caption}
\captionsetup{font=footnotesize}

\begin{document}
% \SweaveOpts{concordance=TRUE}

% \title{A bootstrap-based comparison of ILI intensity thresholds from the moving epidemic and WHO methods}
\title{A statistical assessment of influenza intensity thresholds from the moving epidemic and WHO methods}
\author{Johannes Bracher$^{1, 2}$, Jonas M. Littek$^1$}
\date{9 June 2023\\ \medskip \small
$^1$Karlsruhe Institute of Technology (KIT), Chair of Statistical Methods and Econometrics\\
$^2$Heidelberg Institute for Theoretical Studies (HITS), Computational Statistics Group\\ \medskip
Email address for correspondence:\\ \url{johannes.bracher@kit.edu}\\ % \medskip
% Postal address: \\ Karlsruhe Institute of Technology, Institute of Economics,\\ Bl\"ucherstrasse 17, 76185 Karlsruhe, Germany
}

\maketitle

% \begin{center}
% \textbf{This manuscript is is a preprint and has not been subject to peer review.}
% \end{center}

% \begin{center}
% \textbf{Running title:} Assessment of influenza intensity thresholds
% \end{center}

\abstract{
\begin{quote}
Monitoring of influenza activity is a key task of public health agencies around the world. Intensity thresholds are a commonly used tool to retrospectively classify season peak intensity and to compare current influenza activity to the peak intensity of past seasons. The resulting classifications into low, medium, high or very high intensity help to inform national risk assessment, public health planning and ressource allocation. Moreover, they feed into global risk analyses like the weekly World Health Organization (WHO) Influenza Update. Two common approaches to compute intensity thresholds, recommended in dedicated WHO guidelines, are the  % A common approach to assess season peak activity qualitatively is via intensity thresholds, which are derived empirically from peak intensities of recent seasons. to make influenza activity comparable across countries and surveillance systems. 
moving epidemic method (MEM) and the WHO method. In both methods, thresholds correspond to quantiles of a normal distribution which has been fitted to a set of potentially transformed historical observations. While an extensive body of literature on practical applications of the MEM and WHO method exists, their statistical properties have not been assessed systematically. In this paper we study the two methods analytically and in a simulation study based on re-sampling of data from French and US routine influenza surveillance. Moreover, straightforward extensions to account for small sample sizes and secular trends are described. We find that under the default settings, both the MEM and WHO method on average classify more season peaks than intended as high or very high intensity. Combining characteristics of both and adding a standard correction for small sample sizes, better-calibrated thresholds can be achieved. Even these, however, have modest sensitivity and positive predictive values. This concerns especially thresholds for the very high intensity level, which should thus be interpreted with care.
\end{quote}
}

% \begin{center}
% \textbf{Extended summary}
% \end{center}

% {\begin{quote}
% \footnotesize Intensity thresholds are a common tool to make influenza activity comparable across different countries and surveillance systems. The moving epidemic method (MEM) and the WHO method are widely used to this end and enable classification of season peak intensity as low, medium, high or very high. The two approaches are similar in that they base thresholds on quantiles of a normal distribution fitted to a reference set of historical observations. However, three methodological differences exist. Firstly, in the MEM the normal distribution is fitted to log-transformed incidence data, while the WHO method by default operates on the original scale. Secondly, the MEM uses more than one observation from each past season, fixing the total number to include into the reference set. The WHO method uses only the highest value from each season. Lastly, in the WHO method, but not the MEM, historical data are by default  smoothed prior to the computation of thresholds. We assess the impact of these choices on thresholds both analytically and in a simulation study. The latter is based on re-sampling of influenza-like illness (ILI) data from France and the US, thus reflecting temperate climate settings. We find that when the normal distribution is fitted to untransformed observations, a rather large proportion of new season peaks are classified as high or very high intensity. This can be mitigated by a logarithmic transformation. When fixing the total number of past observations included in the reference set as in the MEM, thresholds increase in expectation the more seasons are available. When only few are available, there is a high chance of classifying new season peaks as high or very high intensity. Smoothing incidence time series prior to computing thresholds results in somewhat less variable estimators, but also a lowering of thresholds. If these are applied to unsmoothed new season peaks, there will again be a large proportion classified as high or very high intensity. If they are applied to smoothed new data, this problem is avoided. In terms of sensitivity and positive predictive values of thresholds, we find that these cannot be expected to exceed rather modest levels if the number of available historical seasons is low; thresholds for very high intensity are particularly affected by this problem. Our practical recommendation is to include one observation per season into the reference set and employ a log transformation in the computation of thresholds. Smoothing can be applied to somewhat reduce the variability of thresholds. However, threholds then should be applied to smoothed rather than raw new peak values, which slightly modifies their interpretation.
% \end{quote}}

\bigskip

\begin{quote}
\textbf{Keywords:} calibration, influenza, intensity threshold, moving epidemic method, re-sampling, sensitivity, WHO method.
\end{quote}

\newpage

%\textbf{Points Moss and H\"ohle:}
%\begin{itemize}
%\item Remove past outliers?
%\item Type of distribution (normal) is problematic
%\item cumulative or peak?
%\item need to add smoothing aspect:
%\begin{itemize}
%\item 3-week in Rguig
%\item 4 weeks in WHO guideline and one other (re-check)
%\end{itemize}
%\end{itemize}
%
%New points:
%\begin{itemize}
%\item discuss general difficulty of estimating extreme values from few observations
%\item mention that it is worse for extreme quantiles
%\item do we classify the "right" seasons as extreme? FPR?
%\item trends / discarding?
%\end{itemize}


% \medskip

% \noindent \textbf{Keywords:} influenza-like illness, intensity thresholds, moving epidemic method, seasonal influenza, WHO method

\section{Introduction}
\label{sec:introduction}

Influenza is a major public health threat and monitoring of influenza activity is a central task of health authorities around the world. In order to strengthen and standardize monitoring in the aftermath of the 2009 influenza H1N1 pandemic, the World Health Organisation (WHO) developed recommendations on severity assessment during seasonal and pandemic outbreaks. According to the PISA guidelines (\textit{Pandemic Influenza Severity Assessment}, \citealt{WHO2014}), severity is defined in terms of three dimensions. \textit{Transmissibility} refers to how many people become ill and is measured e.g., via weekly case numbers of influenza-like illness (ILI) or the percentage of such cases among all consultations by general practitioners. \textit{Severity} is commonly assessed by ratios of recorded deaths and hospitalizations or hospitalizations and cases. Lastly, \textit{impact} can be measured e.g., by weekly case numbers of severe acute respiratory disease (SARI) or admissions to intensive care units.

A key role in the WHO PISA guideline is taken by influenza intensity thresholds. For transmissibility and impact indicators, they represent a generic tool to classify the peak intensity of an influenza season in light of recent peak values. Typically, classifications into \textit{low}, \textit{medium/moderate}, \textit{high} and \textit{very high/extraordinary} intensity are provided. The rationale is that
\begin{quote}
\textit{``about 50--60\% of the season peaks should be above the moderate threshold, $\pm 10\%$ above the high threshold and $\pm 2.5\%$ above the extraordinary threshold''} \citep[p.10]{WHO2017}.
\end{quote}
Similarly to the idea of a ``100-year-flood'' used to communicate hydrological risks \citep{Holmes2010}, the definition of high and very high intensity influenza seasons thus implies that on average they occur every 10 and 40 years, respectively.

Intensity thresholds form part of monitoring procedures in numerous countries, contributing to national-level risk appraisal and planning; see e.g., \cite{CDC2024} on their use in the United States. In addition, intensity classifications feed into international situation assessments by organizations like WHO or the European Center for Disease Prevention and Control (ECDC). In this context, they facilitate comparisons and summaries across countries and surveillance systems. In practice, two uses of thresholds can be distinguished \citep{CDC2024}:
\begin{itemize}
\item[(1)] In the aftermath of an influenza season, thresholds can be used to classify its peak intensity. As an example, the top panel of Figure \ref{fig:maps} shows a retrospective assessment of the 2022/23 season by the US Centers for Disease Prevention and Control (CDC; \citealt{White2023}). In terms of influenza-associated outpatient visits, hospitalization rates, and deaths among children and adolescents, this season was judged as high intensity.
\item[(2)] While an influenza season is ongoing, values of relevant indicators can be compared to intensity thresholds on a weekly basis. We note that the resulting classification is relative to past peaks rather than typical values at a given time of the season. This use of thresholds is illustrated in the bottom panel of Figure \ref{fig:maps}. In this display, published by WHO and ECDC on the \textit{FluNewsEurope} website (\url{https://flunewseurope.org/}), influenza activity across Europe is summarized in a map and a heat chart.
\end{itemize}
%As will be detailed in Section \ref{sec:definitions}, the definition and desired statistical properties of thresholds refer exclusively to season peak values. Retrospective classification of seasons is thus well-aligned with their original purpose. The use in real time, on the other hand, can be seen as a pragmatic monitoring tool, and it needs to be kept in mind that the classification is relative to past peak activity rather than usual activity at a given time of the year.
% While thresholds are designed to assess season peak intensities, they are also used to monitor spatial and temporal patterns over the course of a season. To this end, intensity classifications are commonly summarized in heat charts and maps; see Figure \ref{fig:maps} for an example from the \textit{Flu News Europe} platform run by the World Health Organization (WHO) and the European Center for Disease Prevention and Control (ECDC). Intensity thresholds thus play an important role in assessing the influenza situation at an international level.

\begin{figure}[h!]
\begin{center}
\fbox{\includegraphics[width=0.65\textwidth]{figure/mmwr_with_heading.png}}

\medskip

\fbox{
\includegraphics[width=0.645\textwidth]{figure/map_plus_table.png}}
\end{center}
\caption{Applications of influenza intensity thresholds by public health agencies. Top: Retrospective intensity classifications for the 2022/23 influenza season in the US by the Centers for Disease Prevention and Control (CDC; \citealt{White2023}). Here, $\text{IT}_{50}, \text{IT}_{90}$ and $\text{IT}_{98}$ denote thresholds for medium, high and very high intensity, respectively. Bottom: Influenza intensity map (week 01/2023) and heat chart (weeks 01--07/2023) as published by \textit{FluNewsEurope} (\url{https://flunewseurope.org/}). The display is collated from multiple figures and the heat chart is cropped (the original display containing an alphabetical list of all countries in the European Region of WHO). Copyright information on both figures is provided at the end of this article. % \textbf{include ``typical'' display with colours somewhere? Redondo Bravo? gov.uk?}
}
\label{fig:maps}
\end{figure}
% Following the 2009 influenza H1N1 pandemic
% The \textit{Review Committee on the Functioning of the International Health Regulations and on Pandemic Influenza (H1N1)} recommended that member states apply and evaluate their methods for severity assessment every year, thus improving their pandemic preparedness \citep[p.118]{WHO2011}.
\noindent Setting (1) is more closely aligned with the definition of thresholds in terms of nominal exceedance probabilities for new peak values, and the remainder of the article will focus on this task. We will return to setting (2) in the discussion.

In the PISA guidelines \citep{WHO2017}, two statistical methods were recommended for influenza intensity thresholding. These are the so-called \textit{WHO method} \citep{WHO2014} and the \textit{moving epidemic method} (MEM; \citealt{Vega2015}). The latter has also been recommended by ECDC (e.g., \citealt{ECDC2017}) and is used by the US CDC \citep{Biggerstaff2017}. As will be detailed in Section \ref{sec:definitions}, the WHO and MEM approaches bear important similarities and can be seen as variations of the same general approach. The two methods have been adopted by many public health agencies (see Section \ref{sec:review}). Their statistical properties, however, have not yet been studied in detail. In the present work, we aim to close this gap and derive some recommendations for the computation of influenza intensity thresholds. We study the MEM and WHO methods in their current form and suggest some extensions based on related disease monitoring methods, notably to account for small sample sizes and secular trends. We obtain some analytical results on their properties, complemented by simulation experiments based on re-sampling of routine surveillance data from France and the US. These indicate that using the recommended default settings, both the MEM and the WHO method tend to produce too low thresholds. Consequently, number of season peaks classified as high or very high intensity is higher than intended. The best behaviour of thresholds is achieved by combining characteristics of the MEM and WHO methods, along with a correction for small sample sizes. Even the recommended configuration, however, is characterized by rather modest sensitivity and positive predictive values unless data on many historical seasons are available. This concerns in particular thresholds for very high intensity, which should thus be interpreted with care. % and should be kept in mind when using both the MEM and WHO method.

The remainder of the article is structured as follows. In Section \ref{sec:desirable_properties} we list desirable properties of a threshold setting procedure, which will guide the discussion in the following. Section \ref{sec:definitions} provides definitions of the MEM and WHO methods, highlighting three differences between these otherwise similar approaches. Section \ref{sec:review} consists of an overview of published applications of the MEM and WHO methods, as well as some background on related disease monitoring methods. In Section \ref{sec:formalization_extension} we re-state the thresholding task as a statistical prediction problem and suggest some extensions to the methodology. This is followed by an examination of statistical properties of the methods in Section \ref{sec:analytical_results}. In Section \ref{sec:simulation} we conduct the simulation study before concluding with a discussion in Section \ref{sec:discussion}. Here we also provide practical recommendations on the different implementation choices.


\section{Desirable properties of intensity thresholding methods}
\label{sec:desirable_properties}

% \textbf{Add temporal structure etc here}

We start by stating properties of a thresholding method for influenza activity which we consider desirable. These will guide the analyses in the remainder of the paper. % Our arguments are based on the previously reproduced characterization of thresholds from the PISA guideline \citep{WHO2017}, which refers to the exceedance of thresholds by season peak values. % In terms of the two associated use cases, this corresponds to setting (1), i.e., the retrospective classification of season peak intensity. Setting (2), where thresholds are applied on a weekly basis over the course of a season, will only be addressed in the discussion.

\begin{description}
\item \textbf{Calibration.} As stated in Section \ref{sec:introduction}, thresholds are defined in a statistical manner -- the respective thresholds should be exceeded by 60\%, 10\% and 2.5\% of season peaks. Threshold setting thus corresponds to determining predictive quantiles for future season peak values, and a major requirement is that these are \textit{calibrated}. We call a thresholding method calibrated if in the long run, the right fractions of seasons are classified as low, medium, high and very high. If, for instance, a method flags new season peaks as very high intensity in considerably more than 2.5\% of the cases, this will hamper the usefulness of thresholds.
\item \textbf{Sensitivity and specificity.} In addition to classifying the right fraction of seasons into the various categories, the individual classifications should be correct. Assuming there is an underlying stationary distribution of season peaks, a peak which is truly among the 2.5\%/10\%/60\% highest peaks should also be flagged as such. Conversely, a peak which is not actually among the 2.5\%/10\%/60\% highest ones should not exceed the respective threshold. This aspect can only be assessed in theory and simulation studies, though; in real-world applications, there is no ``gold standard'' to determine the true intensity level of a season peak. % As a complementary perspective to sensitivity and specificity, confusion matrices can be considered to assess how peaks which are truly very high, high, medium or low intensity are classified. % The true distribution of season peaks is of course unknown in practice, but the sensitivity and specificity of a given method can be studied theoretically and in simulation studies.
\item \textbf{Stability.} Estimated thresholds should not be overly variable. Ideally they would not fluctuate strongly around the corresponding quantiles of the true distribution of season peaks.
\item \textbf{Simplicity.} To ensure broad practical applicability, understanding the thresholding method should not require advanced statistical training. The method should be simple enough that a quantitatively literate user can develop a good intuition of its functioning and parameters.
\item \textbf{Ease of practical application.} Methods should be straightforward to apply using well-documented and, ideally, open-source software packages.
\end{description}

\noindent  % while the overarching goal of the \textit{WHO PISA} guidelines is to enhance preparedness for \textit{pandemic influenza}, the intended statistical properties of the MEM and WHO methods refer to \textit{seasonal influenza waves}. The rationale is that a good understanding of the range of peak intensities of seasonal influenza is the basis for a meaningful assessment of pandemic influenza outbreaks. Our reasoning in the following therefore focuses on the setting of subsequent waves of seasonal influenza.
Note that we only address the question of peak intensity thresholds, but not baseline thresholds to determine the season onset \citep{Vega2013}; some analyses on these can be found in \cite{Pang2023}. Also, we do not address the question of classifying peak intensity in real time, i.e., before the season peak has been reached or before it is clear whether a peak has been reached. % Rather than simple thresholds, this would require control charts (see e.g., \citealt{Liu2019}), which we consider outside the scope of the present article.


\section{Definition of the MEM and WHO methods}
\label{sec:definitions}

While framed slightly differently in the respective documentations, the MEM and WHO methods are two special cases of the same general approach. We assume that thresholds are based on weekly values of a transmissibility or severity indicator (see Section \ref{sec:introduction}) from $m$ past seasons and applied to the ($m + 1$)-th season.  We denote the value for season $j$ and week $k$ by $x_{j, k}$, with more specific notation introduced below. Implicitly it is assumed that each season consists of just one wave without multiple peaks separated by longer time spans. This is typically the case in temperate, but not necessarily in tropical regions. Thresholds are then obtained via the following steps.

\begin{enumerate}
\item[(a)] \textbf{Smoothing of historical data} (optional): apply an $l$-week moving average to all historical seasons. If data are smoothed, we denote by $x^\text{raw}_{j, k}$ the raw observation from season $j = 1, \dots, m$, week $k = 1, \dots, 52$, and by
$$
x^\text{smo}_{j, k} = \frac{1}{l} \sum_{d = 0}^{l - 1} x^\text{raw}_{j, k - d}, \ \ k = l, \dots, 52
$$ the smoothed version. In the remainder of this description we denote whichever of $x^\text{raw}_{j, k}$ and $x^\text{smo}_{j, k}$ is used to compute thresholds by $x_{j, k}$.
\item[(b)] \textbf{Sorting:} Within each historical season $j = 1, \dots, m$ order all observations $x_{j, k}$ in decreasing order, denoting the $i$-th largest observation from season $j$ by $x^{(i)}_j$. More generally we will use the notation $^{(\cdot)}$ to denote an ordering of values, e.g., denoting by $x^{\text{raw}, (i)}_j$ the $i$-th largest unsmoothed incidence value from season $j$.
\item[(c)] \textbf{Selection of reference set:} Select the $n$ largest observations from each of the $m$ past seasons to construct a reference set $\mathcal{X} = \{x_j^{(i)}: j = 1, \dots, m; i = 1, \dots, n\}$.
\item[(d)] \textbf{Data transformation:} Apply a monotonically increasing transformation $y_j^{(i)} = f(x_j^{(i)})$ to all members of the reference set $\mathcal{X}$ to obtain a reference set $\mathcal{Y}$ of transformed historical observations.
\item[(e)] \textbf{Fitting a normal distribution:} Assume that the transformed values in $\mathcal{Y}$ come from a normal distribution and compute estimates of its mean and variance,
\begin{align}
\begin{split}
\bar{y} & = \frac{\sum_{j = 1}^m\sum_{i = 1}^n y_j^{(i)}}{n\times m}\\
\sd^2 & = \frac{\sum_{j = 1}^m\sum_{i = 1}^n \left(y_j^{(i)}  - \bar{y}\right)^2}{n\times m - 1}.
\end{split}\label{eq:moments}
\end{align}
\item[(f)] \textbf{Computation of thresholds:} Define intensity thresholds on the transformed scale as quantiles of the normal distribution N$(\bar{y}, \sd^2)$, i.e.\ compute
\begin{equation}
\hat{q}_{Y, \alpha} = \bar{y} + z_\alpha \sd, \label{eq:def_q}
\end{equation}
with $z_\alpha$ the $\alpha$ quantile of the standard normal distribution. The $\hat{q}_{Y, \alpha}$ can be seen as estimates of quantiles $q_{Y, \alpha}$ of an underlying distribution of peak values. The default choices are
\begin{itemize}
\item[(i)] the 40th percentile $\hat{q}_{Y, 0.4} = \bar{y} - 0.25 \sd$ as the threshold for medium intensity;
\item[(ii)] the 90th percentile $\hat{q}_{Y, 0.9} = \bar{y} + 1.28 \sd$ as the threshold for high intensity;
\item[(iii)] the 97.5th percentile $\hat{q}_{Y, 0.975} = \bar{y} + 1.96\sd$ as the threshold for very high intensity.
\end{itemize}
As we will detail in Section \ref{subsec:reformulation}, we will moreover consider an alternative formulation based on the $t$-distribution,
\begin{equation}
\hat{q}_{Y, \alpha} = \bar{y} + t_{m\times n - 1, \alpha} \times \sqrt{1 + \frac{1}{m\times n}} \times s,
\label{eq:q_Y_t}
\end{equation}
with $t_{m\times n - 1, \alpha}$ the $\alpha$ quantile of the $t$ distribution with $m\times n - 1$ degrees of freedom.
\item[(g)] \textbf{Transformation of thresholds to the original scale:} Obtain thresholds on the original scale by applying the inverse transformation, i.e. for $\alpha = 0.4, 0.9, 0.975$ set% $\hat{q}_{X, 0.4} = f^{-1}(\hat{q}_{Y, 0.4})$, $\hat{q}_{X, 0.9} = f^{-1}(\hat{q}_{Y, 0.9})$, $\hat{q}_{X, 0.975} = f^{-1}(\hat{q}_{Y, 0.975})$.
$$
\hat{q}_{X, \alpha} = f^{-1}(\hat{q}_{Y, \alpha}).
$$
\item[(h)] \textbf{Application of thresholds:} The thresholds are applied to classify the peak value of the $(m + 1)$-th season. Depending on the exact specification, thresholds can be applied either to the raw peak value $x_{m + 1}^{\text{raw}, (1)}$ or the smoothed version $x_{m + 1}^{\text{smo}, (1)}$.
\end{enumerate}

\noindent The MEM and WHO methods are special cases of this procedure, see also overview in Table \ref{tab:differences} and the graphical illustration in Figure \ref{fig:illustration}. In the MEM method, no smoothing is applied and the default data transformation $f$ is the natural logarithm.  \cite{Vega2015} recommend to use $5 \leq m \leq 10$ seasons to ensure a recent data basis. The number of observations included per season is set to $n = 30/m$, rounded to the nearest integer (with a minimum value of $n = 1$). The total number of historical observations is thus kept approximately fixed. This description corresponds to that by \cite{Vega2015} and is also reflected in the default settings of the R package \texttt{mem} \citep{Lozano2020}. The package, however, permits the user to choose $n$, $m$, and $f$ (\texttt{i.type.intensity	= 5} for no transformation, \texttt{i.type.intensity = 6} for the log transformation). We note that while \texttt{mem} does not currently allow for data smoothing using a moving average, alternatives like LOESS are available (locally estimated scatterplot smoothing; used e.g., by \citealt{Wang2023}). % The term \textit{moving epidemic method} could thus also be used as an umbrella term for the general procedure described above. We here use it in a more narrow sense for the specification from \cite{Vega2015}, reflected in the default settings of the \texttt{mem} package.
% \begin{itemize}
% \item

The implementation of the WHO method in the publicly available \textit{WHO Average Curves} Shiny Web App \citep{WHO2023} likewise offers a lot of flexibility. Our description is based on the default settings as of May 2023 and the description in \cite{WHO2014}. Smoothing of data prior to the computation of thresholds is recommended, with a default of $l = 3$ (adapted from $l = 4$ in \citealt[p68]{WHO2014}). Subsequently, $n = 1$ observation per season is used and by default no transformation is applied to the reference set. If peak incidences vary strongly across seasons, a log transformation is recommended. At least $m = 3$ historical seasons are required, but it is noted that the ``accuracy of these thresholds should be expected to increase with the number of seasons of good quality data available'' \cite[p22]{WHO2023}. Thresholds are by default applied to unsmoothed new peak values.


\begin{figure}
\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/illustration_mem_who.pdf}
\end{center}
\caption{Illustration of the MEM and WHO methods with their respective default settings, computed using $m = 6$ historical seasons (solid lines) and applied to a seventh (dotted lines). Here, both methods classify the new season peak as medium intensity. Observations included in the reference set are highlighted by dots (with $n = 30/6 = 5$ for MEM, $n = 1$ smoothed values for WHO). The distribution fitted to the reference set is shown as a vertical density plot. Note that for the MEM this is a log-normal as the normal distribution is fitted to log-transformed values. The data used for illustration correspond to the years 2012--2019 from the French region of Grand Est, see Section \ref{subsec:data} and Figure \ref{fig:data}.}
\label{fig:illustration}
\end{figure}


Both the inclusion of multiple observations per season in the MEM ($n > 1$) and the smoothing of data in the WHO method ($l > 1$) can be seen as attempts to base thresholds on more data than just one peak value per historical season. This is intended to make estimation more stable. The impact of these strategies on the resulting thresholds will be discussed in Section \ref{sec:analytical_results}.


% \item

% \end{itemize}

\begin{table}[h]
\caption{Default settings of the moving epidemic and WHO methods.}
\label{tab:differences}
\begin{center}
\footnotesize
\begin{tabular}{lll}
\toprule
& moving epidemic method & WHO Method \\
\midrule
smoothing of historical data & none & moving average, $l = 3$\\
number $n$ of observations used per season & $n = \max[\text{round}(30/m), 1]$ & $n = 1$\\
default transformation for reference set & natural logarithm & none\\
recommended number $m$ of historical seasons & $5 \leq m \leq 10$ & $m \geq 3$, more recommended\\
smoothing of new season peak & none & none\\
\bottomrule
\end{tabular}
\end{center}

\end{table}



% \noindent In the following we will study analytically and empirically how these different analytical choices impact the resulting thresholds.

% \subsection{Hypothesized properties}

\section{Related literature}
\label{sec:review}

\subsection{Use cases of the MEM and WHO method}


To improve our understanding of the different settings in which the MEM and WHO methods are applied in practice we performed a literature search of articles published in English language and citing the papers \cite{Vega2015}, \cite{WHO2014} and \cite{WHO2017} until May 2023 (identified via \textit{CrossRef} and \textit{Google Scholar}). The results are summarized in Table \ref{tab:literature}.

As can be seen from the large number of entries from the years 2019 and 2020, the MEM has quickly become a standard approach in the determination of intensity thresholds for influenza and other respiratory diseases. The articles come from numerous countries and in many cases have been co-authored by representatives of national or regional public health agencies. In most analyses, threshold levels at the 40th, 90th and 97.5th percentile as in Section \ref{sec:definitions} are used. % An interesting exception is the paper by \cite{Domegan2022} who introduced an additional \textit{super-extraordinary} threshold (though without specifying the corresponding quantile level).
 Variability with respect to the number $m$ of historical seasons included is considerable, with a range from 3 to 16 seasons. Consequently, the number $n$ of observations included per season ranges from two to ten. Only one of the reviewed papers \citep{Dahlgren2022} indicated a modification of the default setting $n = 30/m$  and used $n = 3$ with $m = 7$.

We only found six published applications of the WHO method, two of which provide a comparison to the thresholds from the MEM method \citep{Rguig2020, Teeluck2021}. In most applications of the WHO method it is not documented whether smoothing was applied and which value was chosen for $l$. In terms of quantile levels, several papers used the 95\% rather than 97.5\% quantile for the highest threshold.

\begin{table}[h!]
\caption{\footnotesize Published applications of the MEM and WHO methods to estimate intensity thresholds; $m$ is the number of historical seasons used, while $n$ is the number of observations used per season (in the bottom table we always have $n = 1$). The window width for smoothing with a moving average is denoted by $l$ (in the top table we always have $l = 1$). ``Percentiles'' indicates which percentiles were used for the medium, high and very high thresholds, with ``?'' indicating no information was found. Abbreviations: ARI/SARI = (severe) acute respiratory infection; ILI = influenza-like illness; RSV = respiratory syncytial virus.}
\label{tab:literature}
\center
% \vspace{-3mm}
\scriptsize
\begin{tabular}{l l l l l l l}
\toprule
\multicolumn{7}{c}{(a) Moving epidemic method}\\
\toprule
Region & Disease / condition & Years & $m$ & $n$ & Percentiles & Authors\\
\midrule
Africa (25 countries) & influenza & varies & varies & varies & 40, 90, 97.5 & \cite{Igboh2021} \\
Australia & ILI/influenza & 2012--2017 & 5 & 6 & 40, 90, 99 & \cite{Vette2018}\\
Australia, Chile, & ILI/SARI & 2013--2019 & 6 & 5 & 40, 90, 97.5 & \cite{Sullivan2019}\\
New Zealand,\\
South Africa\\
Bangladesh & influenza & 2016--2019 & 4 & 8 & 40, 90, 97.5 & Aba Oud et al (\citeyear{AbaOud2021})\\
Catalonia & ILI & 2010--2016 & 5 & 6 & ? & \cite{Basile2018}\\
Catalonia & ILI & 2005--2018 & 12 & 3 & ? & \cite{Basile2019}\\
Catalonia & ILI/influenza & 2010--2017 & 7 & 4 & ? & \cite{Torner2019}\\
Egypt & SARI/ILI & 2010--2017 & 6 & 5 & 40, 90, 97.5 & \cite{AbdElGawad2020}\\
Egypt & SARI & 2013--2015 & 3 & 10 & ? & \cite{Elhakim2019}\\
England & ILI & 2010--2016 & 6 & 5 & ? & \cite{Wagner2018}\\
Finland & influenza & 2011--2016 & 5 & 6 & ? & \cite{Pesaelae2019}\\
Guangdong, China & influenza & 2011-2018 & 7 & 4 & 40, 90, 97.5 & \cite{Kang2021} \\
Hubei, China & influenza & 2010--2018 & 8 & 4 & 40, 90, 97.5 & \cite{Jiang2022} \\
Ireland & numerous, incl. ILI, & 2015--2019 & 5 & 6 & ? & \cite{Domegan2022}\\
& all-cause mortality\\
Maritius & ARI & 2012--2018 & varies$^\dagger$ & varies & 40, 90, 97.5 & Teeluck et al (\citeyear{Teeluck2021}) \\
Mexico & short-term& 2015--2019 & 5 & 6 & 40, 90, 97.5 & Hernandez-Avila\\
& disability claims & & & & & et al (\citeyear{HernandezAvila2022})\\
Montenegro & ILI & 2010--2018 & 7 & 4 & 40, 90, 97.5 & \cite{Rakocevic2019}\\
Morocco & ILI & 2005--2017 & 11 & 3 & 40, 90, 97.5 & \cite{Rguig2020}\\
Netherlands & RSV & 2005--2017 & 12 & 3 & 40, 90, 97.5 & \cite{Vos2019}\\
New Zealand & SARI, ILI, influenza & 2015-2019 & 5 & 6 & 40, 90, 97.5 & \cite{Wood2021}\\
Norway & influenza & 2006--2015 & 9 & 3 & ? & \cite{Benedetti2019}\\
Pakistan & ILI, SARI & 2008--2017 & 10 & 3 & 40, 90, 97.5 & \cite{Nisar2020}\\
Portugal & ILI & 2012--2017 & 5 & 6 & 40, 90, 97.5 & \cite{Pascoa2018}\\
Russia & influenza & 2015--2022 & 7 & 4 & ? & \cite{Sominina2022}\\
Scotland & influenza & 2010--2018 & 7 & 4 & ? & \cite{Murray2018}\\
Scotland & influenza & 2010--2019 & 7--8 & 4 & 40, 90, 97.5 & \cite{Dickson2020}\\
Slovenia & RSV & 2008--2018 & 10 & 3 & 40, 90, 97.5 & \cite{Grilc2021}\\
Spain (17 regions) & ILI & 2003--2015 & 4--10 & 3--8 & 40, 90, 97.5 & \cite{Bangert2017}\\
Spain & ILI & 2001--2018 & 16 & 2 & 40, 90, 97.5 & \cite{RedondoBravo2020}\\
Sweden & influenza & 2008--2019 & 12 & 3 & ? & \cite{Spreco2020}\\
Tunisia & ILI & 2009-2018 & 9 & 3 & 50, 90, 95 & \cite{Bouguerra2020}\\
United Kingdom & ILI & 2000--2013 & 10 & 3 & 40, 90, 97.5 & \cite{Green2015}\\
United Kingdom & ILI/RSV & 2011--2018 & 4--6 & 5--8 & 40, 90, 97.5 & \cite{Harcourt2019}\\
USA & ILI/influenza & 2003--2015 & 11 & 3 & 50, 90, 98 & \cite{Biggerstaff2017}\\
USA & ILI & 2010--2015 & 5 & 6 & 50, 90, 98 & \cite{Dahlgren2018}\\
USA & influenza & 2010--2016 & 6 & 5 & 50, 90, 98 & \cite{Dahlgren2019}\\
USA & all-cause mortality & 2013--2020 & 7 & 3 & 50, 95, 99.5 & \cite{Dahlgren2022}\\
Various (17 countres) & RSV & 2015--2020 & 5 & 6 & 40, 90, 97.5 & \cite{Wang2023} \\ % use smoothing
\midrule 
\multicolumn{7}{c}{(b) WHO method}\\
\toprule
Region & Disease & years covered & $m$ & $l$ & percentiles & authors\\
\midrule
Cambodia & ILI & 2009--2015 & 7 & ? & mean, 90, 95 & \cite{Ly2017}\\
Maritius & ARI & 2012--2018 & varies$^1$ & ? & 40, 90, 97.5 & Teeluck et al (\citeyear{Teeluck2021}) \\
Morocco & ILI & 2005--2017 & 11 & 3 & 40, 90, 97.5 & \cite{Rguig2020}\\
Philippines & ILI & 2006--2012 & 7 & 4 & 90 & \cite{Lucero2016}\\
Tokyo, Japan & influenza & 2014--2018 & 5 & ? & 90, 95 & \cite{Matsuda2022} \\
Victoria/Australia & ILI & 2002--2011 & 6--10 & ? & 90, 95 & \cite{Tay2013}\\
\bottomrule
\multicolumn{7}{c}{$^\dagger$ Teeluck at al (\citeyear{Teeluck2021}) use various methods to split up multi-peak seasons, leading to different effective numbers of waves used.}\\ 
\end{tabular}
\end{table}


\subsection{Related disease monioring methods}
\label{sec:related_literature}


There is a vast literature on monitoring tasks in infectious disease epidemiology and public health. Well-known approaches include Shewhart, cumulative sum (CUSUM, \citealt{Hoehle2008}) and exponentially weighted moving average (EWMA, \citealt{Steiner2010}) control charts. Time series methodology can likewise be employed, see e.g., \cite{Reis2003} for an application of ARIMA models. For a comprehensive account we refer the reader to the review aricles by \cite{Allevius2020}, \cite{Rigdon2015} and \cite{Unkel2012}. In the following we focus on two widely used approaches with close links to the MEM and WHO methods.

A simple Shewhart-type method is used by the US CDC as part of the \textit{EARS} system (Early Aberration Reporting, \citealt{Hutwagner2003}). Denote by $y_1, \dots, y_m$ the $m$ most recent values of a (typically daily) epidemiological indicator. An upper alarm threshold for a new observation $y_{m + 1}$ is given by
$$
U = \bar{y} + 3 \times s,
$$ 
where, similarly to equation \eqref{eq:moments} we set $\bar{y} = 1/m \times \sum_{j = 1}^m y_j$ and $s^2 = 1/(m - 1) \times \sum_{j = 1}^m (y_j - \bar{y})^2$. This closely resembles the WHO method, i.e., the procedure from Section \ref{sec:definitions} with $n = 1$ and $f$ the identity function. The multiplier 3 corresponds to $\alpha = 0.9987$. This approach was conceived as a ``drop -in'' surveillance method, i.e., a method to be used in the short term and based on little historical data. In practice, $m = 7$ observations are used to compute thresholds, which is similar to the typical number of historical seasons used in intensity thresholding.

The EARS approach can also be seen as an intercept-only linear regression model
$$
Y_{t} = \beta_0 + \varepsilon_{t}, \ \ \ \ \varepsilon_t \sim \text{N}(0, \sigma_\varepsilon^2),
$$
with $\bar{y}$ and $s^2$ as estimators of $\beta_0$ and $\sigma_\varepsilon^2$. Indeed, regression-based approaches represent a flexible and general monitoring framework, compare Section 2 in \cite{Unkel2012}. Classic parametric approaches like the \textit{Serfling method} \citep{Serfling1896} are widely used, see e.g., \cite{Thompson2009} for an application to influenza mortality. Here, a regression model including time trends and sinusoidal functions for seasonality, say,
\begin{equation}
Y_{t} = \beta_0 + \beta_1 t + \beta_3 \sin(2\pi t / \omega) + \beta_4 \cos(2\pi t / \omega) + \varepsilon_t, \ \ \ \varepsilon_t \sim \text{N}(0, \sigma_\varepsilon^2),
\label{eq:serfling}
\end{equation}
is fitted to historical values $y_1, \dots, y_m$ spanning multiple seasons. The period of seasonal cycles is given by $\omega$, with $\omega = 52$ for weekly data and yearly seasonality. The model fit subsequently serves to generate a prediction interval for a new value $y_{m + 1}$. The equally popular \textit{Farrington method} is conceptually similar, but adapts to low-count settings via suitable data transformations \citep{Farrington1996} or count-valued response distributions \citep{Noufaily2013}. While technically a generalization of the EARS approach, the rationale of the Serfling and Farrington methods is somewhat different. As already noted by \cite{Hutwagner2003}, the latter aim to detect deviations from past seasonal patterns while accounting for long-term trends. EARS, on the other hand, targets abrupt changes relative to recent incidence levels.

 
%\begin{itemize}
%\item outbreak detection
%\item mortality: euromomo
%\item pharmakovigilance
%\item process / statistical quality control \url{https://link.springer.com/chapter/10.1007/978-3-7908-2674-6_14}
%\item environmental monitoring: quarterly observations of chemical concentrations in groundwater (see Millard book)
%\item scan statistics
%\item Serfling
%\end{itemize}
%
%\begin{itemize}
%\item Mention that in different contexts different things are taken into the condition -- definition of the reference is key
%\item mention sort of shared regression approach. Link back to that later on
%
%\end{itemize}



\section{Statistical formalization and extension}
\label{sec:formalization_extension}

\subsection{Formulation as a prediction task}
\label{subsec:reformulation}

We now re-state the purpose and assumptions of the WHO and moving epidemic methods in statistical terms, denoting random variables by capital letters (e.g., $X_j^{(i)}$) and their realizations by lower-case letters (e.g., $x_j^{(i)}$). The goal of the thresholding exercise is to use data from seasons 1 through $m$ to obtain values $\hat{q}_{Y, \alpha}$ which shall be exceeded by a new transformed peak value $Y_{m + 1}^{(1)}$ with probability $(1 - \alpha) \in \{0.6, 0.1, 0.025\}$, respectively. The $\hat{q}_{Y, \alpha}$ can thus be seen as \textit{predictive quantiles} at levels $\alpha \in \{0.4, 0.9, 0.975\}$. Prediction of new realizations based on parametric assumptions and a set of previous observations is a well-studied problem; see e.g., \cite{Millard2013} on applications in environmental studies. We note that in \cite{WHO2014} and \cite{Vega2015} the thresholds are refered to as the upper ends of one-sided confidence intervals. This, however, is imprecise terminology as in the computations the standard deviation $\sd$ rather than the standard error $\sd/\sqrt{nm}$ is used (see documentation of the \texttt{mem} package and \citealt{WHO2014}, p.69).

Implicitly, both the WHO and the moving epidemic method assume that the elements of $\mathcal{Y}$ are independent and identically distibuted draws from a Gaussian distribution, and that a new peak value $Y_{m + 1}^{(1)}$ comes from that same distribution. As we will examine different violations of this overall assumption, we state its various aspects in more detail. To this end denote by
\begin{equation}
\tilde{\mathbf{Y}}_j = (Y^{(1)}_j, \dots, Y^{(n)}_j)^\top\label{eq:Y_tilde}
\end{equation}
the random vector of the $n$ largest transformed incidence values from season $j$ in decreasing order. The set $\mathcal{Y}$ thus results from pooling $\tilde{\mathbf{Y}}_1, \dots, \tilde{\mathbf{Y}}_m$. We then distinguish the following assumptions.
\begin{itemize}
\item[\textbf{(H)}] It is assumed that the elements of $\tilde{\mathbf{Y}}_1, \dots, \tilde{\mathbf{Y}}_{m + 1}$ are \textit{homogeneous} in two ways.
\begin{itemize}
\item[\textbf{(HB)}] \textit{Between-season} homogeneity is assumed, meaning that the distribution of $\tilde{\mathbf{Y}}_j$ is the same for all $j = 1, \dots, m + 1$. Secular time trends and other changes in seasonal patterns are thus excluded.
\item[\textbf{(HW)}] Moreover, \textit{within-season} homogeneity is assumed in the sense that the elements $Y_j^{(1)}, \dots, Y_j^{(n)}$ of each vector $\tilde{\mathbf{Y}}_j, j = 1, \dots, m$ all follow the same distribution.
\end{itemize}
\item[\textbf{(I)}] Two \textit{independence} assumptions paralleling (HB) and (HW) are made.
\begin{itemize}
\item[\textbf{(IB)}] The vectors $\tilde{\mathbf{Y}}_1, \dots, \tilde{\mathbf{Y}}_{m + 1}$ are assumed to be independent. We thus do not allow for for correlations between subsequent seasons.
\item[\textbf{(IW)}] The elements $Y_j^{(1)}, \dots, Y_j^{(n)}$ within a vector $\tilde{\mathbf{Y}}_j$, i.e., the $n$ highest observations from the same season $j$, are assumed to be independent.
\end{itemize}
\item[\textbf{(N)}] It is assumed that the transformation $f$ leads to multivariate \textit{normality} of the vectors $\tilde{\mathbf{Y}}_j, j = 1, \dots, m$.
\end{itemize}
Whether assumptions (HB) and (IB) are reasonable is largely an empirical question. Assumptions (HW) and (IW), on the other hand, are almost inevitably violated if $n > 1$, as by construction $Y_j^{(1)} \geq Y_j^{(1)} \geq \dots \geq Y_j^{(n)}$. Technically, assumption (N) is likewise incompatible with this constraint, but may be helpful in practice.  For the time being we nonetheless accept all of the above assumptions and state standard theory on predictive quantiles in this setting. For large $m\times n$, the normal approximation
$$
\hat{q}_{Y, \alpha} = \bar{y} + z_\alpha \sd
$$
as in equation \eqref{eq:def_q} can be used. A common rule of thumb is to require $m \times n \geq 30$ (which seems to be the reasoning behind the MEM default setting $n = 30/m$). For small $m \times n$, however,
\begin{equation*}
\hat{q}_{Y, \alpha} = \bar{y} + t_{m\times n - 1, \alpha} \times \sqrt{1 + \frac{1}{m\times n}} \times s
\end{equation*}
as in equation \eqref{eq:q_Y_t} should be used (see e.g., \citealt{Preston2000}). The $t$-distribution with $m \times n$ degrees of freedom here accounts for the fact that the standard deviation $s$ needs to be estimated along with $\bar{y}$. As illustrated in Figure \ref{fig:illustration_t}, the predictive quantiles, and thus intensity thresholds, resulting from \eqref{eq:def_q} and \eqref{eq:q_Y_t} can differ substantially, with the latter leading to higher values for $\alpha = 0.9, 0.975$. We note that \cite{Allevius2020} have brought forward the same argument with respect to the EARS method.


\begin{figure}
\begin{center}
\includegraphics[width = 0.8\textwidth, trim={0 5mm 0 0},clip]{figure/illustration_t_normal.pdf}
\end{center}
\caption{Comparison of thresholds based on the normal and $t$-distribution (using $m = 6, n = 1$ and a log transformation). The data used for illustration correspond to the years 2012--2019 from the French region of Grand Est, see Section \ref{subsec:data} and Figure \ref{fig:data}.}
\label{fig:illustration_t}
\end{figure}


% In either case, the estimation of the $q_{Y, \alpha}$ hinges on the quantities $\bar{y}$ and $s$. In the two following sections we will assess how these are affected by the implementation choices of the moving epidemic and WHO methods.

% Assuming stationarity of the underlying process, i.e., that $Y_{m + 1}^{(1)}$ follows the same distribution as available historical observations, standard theory is available to construct central prediction intervals, or equivalently, predictive quantiles. Assuming that the empirical mean $\bar{y}$ and standard deviation $s$ have been computed from $m \times n$ independent and identically distributed samples

% Both the MEM and WHO method essentially serve to estimate quantiles of the distribution of season peak values ($q_{X, 0.4}, q_{X, 0.9}, q_{X, 0.975}$) from historical data. In the following we obtain some analytical results on how the estimators ($\hat{q}_{X, 0.4}, \hat{q}_{X, 0.9}, \hat{q}_{X, 0.975}$) behave under different variations of the procedure outlined in Section \ref{sec:definitions}. In all cases we will simplifyingly assume that the available historical seasons are independent realizations of the same random process, i.e., we ignore time trends, correlations between neighbouring seasons and other changes in seasonal patterns. To highlight that we treat all incidence values as random rather than observed variables in this section, we will denote them by capital rather than lowercase letters (e.g., $X_j^{(i)}$ rather than $x_j^{(i)}$). All derivations are provided in Supplement \ref{appendix:derivations}.


\subsection{Accounting for secular trends}
\label{subsec:regression}
\label{subsec:trends}

As mentioned in Secion \ref{sec:related_literature}, regression approaches are a natural extension of moment-based techniques like the WHO and moving epidemic method. We illustrate this with a suggestion on how to account for secular trends in threshold setting, thus relaxing assumption (HB). Secular trends are the main reasons why for the MEM it is recommend to use no more than $m = 10$ seasons of historical data, with \citeauthor{Vega2013} (\citeyear{Vega2013}, p.556) cautioning that ``more than ten seasons may further increase accuracy but make a model susceptible to biases from secular trends.'' An obvious model accouning for a secular trend is
\begin{equation}
Y_j^{(1)} = \beta_0 + \beta_1 \times j + \epsilon_j, \ \ \ \ \varepsilon_j \stackrel{\text{i.i.d.}}{\sim} \text{N}(0, \sigma^2_\varepsilon).\label{eq:trend}
\end{equation}
Parameter estimates $\hat{\alpha}$ and $\hat{\beta}$ can be computed using peak values $Y_1^{(1)}, \dots, Y_m^{(1)}$ from the last $m$ seasons and the ordiary least squares method. An unbiased estimator of ${\sigma}^2_\varepsilon$ is
$$
\hat{\sigma}^2_\varepsilon = \frac{1}{m - 2} \sum_{j = 1}^m (y_i - \beta_0 - \beta_1 \times j)^2.
$$
Predictive quantiles at levels $\alpha = 0.4, 0.9, 0.975$, and thus intensity thresholds, can be obtained using standard methods implemented in statistical software packages such as R. As we show in Supplement \ref{suppl:regression}, the generalization of equation \eqref{eq:q_Y_t} resulting for model \eqref{eq:trend} is of a rather simple form. A graphical illustration of the resulting time-varying thresholds can be found in Figure \ref{fig:illustration_trend}. 

\begin{figure}[h!]
\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/illustration_trend.pdf}
\end{center}
\caption{Illustration of thresholds accounting for a secular trend (using $n = 1$, a log transformation and the $t$-distribution). The data used for illustration correspond to the years 1986--2019 from the French region of Grand Est, see Section \ref{subsec:data} and Figure \ref{fig:data}. The pandemic season 2009/2010 has been omitted.}
\label{fig:illustration_trend}
\end{figure}

This approach could easily be extended to accommodate $n > 1$ values from each historical season. However, as we will argue in Section \ref{subsec:choice_n}, setting $n > 1$ will lead to biased thresholds, unless a more flexible model formulation accounting for relevant correlation structures is adopted. We therefore do not pursue this approach here.

An obvious question is under which conditions thresholds should be corrected for secular trends. In the \cite{Farrington1996} method, a trend is included if at least three years of weekly data are available and the trend parameter is significant at the 5\% level. This, however, does not seem practical in intensity thresholding where historical peak values are scarce. A closely related question is how much data is needed to reliably fit a regression model with a given number of parameters. Model \eqref{eq:trend} is discussed in \cite{Hyndman2007}; however, the authors stress that there cannot be a general answer as everything depends on the variance of $\varepsilon_j$ relative to the strength of the trend $\beta$ (both of which are not known in advance in practice). We will return to this question in Section \ref{subsec:results_trends} and the Discussion section.


% Vega 2012: We selected countries with five or more years of historical data to be included in this study, but it is still open how many historical seasons should be included in building the model, even when long time series are available. Changes in case reporting, demographics, case definitions and secular trends (which are declining in several countries) 39 could affect how well the model fits the tested data. Typically, influenza mortality models make use of at least the five preceding seasons. 13,40,41 More than ten seasons may further increase accuracy but make a model susceptible to biases from secular trends.
 
\section{Statistical properties}
\label{sec:analytical_results}

In this section we provide some statistical reasoning on the consequences of various implementation choices within the thresholding framework outlined in Section \ref{sec:definitions}. Application-focused readers may find the simulation-based illustration of our findings in Section \ref{sec:simulation} more vivid. % A high-level summary of the discussed findings can also be found in the introduction. % In summary, in the following subsections we demonstrate that there are three sources of bias, all of which lead to high and very high thresholds being exceeded more often than intended. (1) using $n > 1$ leads to a downward bias in high and very thresholds, which as a consequence are exceeded too often. (2) If smoothing is applied to historical data, the same should be done to new peaks as otherwise thresholds are again biased downwards. (3) Thresholds based on the normal distribution are biased

\subsection{Basing thresholds on normal rather than $t$-distributions}
\label{subsec:normal_vs_t}

As discussed in Section \ref{subsec:reformulation} and previously by \cite{Allevius2020}, statistical theory implies that quantiles of a suitable $t$-distribution need be used to obtain valid predictive quantiles for normally distributed outcomes. Using quantiles of a normal distribution as in the original threshold definition \eqref{eq:def_q} will lead to distorted exceedance probabilities unless $m \times n$ is large. Under assumptions (H), (I) and (N), thresholds \eqref{eq:def_q} will be exceeded by a new season peak with probability
$$
1 - F^t_{m\times n - 1}\left[\Phi^{-1}(\alpha)/\sqrt{1 + 1/(m \times n)}\right]
$$
rather than $1 - \alpha$. Here, we denote by $F^t_{mn - 1}$ the cumulative distribution function of the $t$-distribution with $m \times n - 1$ degrees of freedom. Figure \ref{fig:calibration_t_normal} displays this relationship for different values of $m \times n$. While for the medium threshold, the exceedance probabilities are slightly too low, the high and very high thresholds are exceeded considerably too often if $m \times n$ is small. For example, if using $m \times n = 10$, the very high threshold is exceeded with probability 5\% rather than 2.5\%. For $m \times n$ this probability increases to 7.4\%. This aspect is most relevant for the WHO method, where the choice $n = 1$ typically leads to small $m \times n$. For $m \times n = 30$ as in the MEM default settings, the differences between the $t$ and the normal distribution are small.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.6]{figure/exceedance_prob_normal.pdf}
\end{center}
\caption{Exceedance probabilities for thresholds based on normal quantiles (equation \eqref{eq:def_q}) if assumptions (H), (I), (N) are fulfilled. For medium, high and very high intensity thresholds, exceedance probabilities are displayed as a function of the number $m \times n$ of observations in the reference set. The respective nominal levels are shown as dotted horizontal lines.}
\label{fig:calibration_t_normal}
\end{figure}

\subsection{Choice of the number $n$ of observations used per season}
\label{subsec:choice_n}


We next consider the impact of the number $n$ of observations used per historical season. We assume that no smoothing is applied in Step (a) of the algorithm described in Section \ref{sec:definitions}. Now consider the vectors $\tilde{\mathbf{Y}}_j, j = 1, \dots, m + 1$ as defined in equation \eqref{eq:Y_tilde}. %the random vector of the $n$ largest transformed incidence values from season $j$ in decreasing order, i.e.
% $$
% \tilde{\mathbf{Y}}_j = (Y^{(1)}_j, \dots, Y^{(n)}_j)^\top.
% $$
We maintain the assumption that $\tilde{\mathbf{Y}}_1, \dots, \tilde{\mathbf{Y}}_m$ are identically (HB) and independently (IB) distributed. However, we abandon the assumptions that their elements are identically (HW) and independently (IW) distributed, as these are at odds with the fact that $Y_j^{(1)} \geq Y_j^{(2)} \geq Y_j^{(m)}$. We denote the theoretical mean and covariance of $\tilde{\mathbf{Y}}_1, \dots, \tilde{\mathbf{Y}}_m$ by
\begin{align}
\mathbb{E}\left(\tilde{\mathbf{Y}}_j\right) = \boldsymbol{\mu} = \left(\begin{array}{c}
\mu_1\\
\vdots\\
\mu_n
\end{array}\right) \ \ \ \text{and} \ \ \ \text{Cov}\left(\tilde{\mathbf{Y}}_j\right) = \boldsymbol{\Sigma} =
\left(\begin{array}{ccc}
\sigma_{1, 1} & \cdots & \sigma_{1, n}\\
\vdots & \ddots &\vdots\\
\sigma_{n, 1} & \cdots & \sigma_{n, n}
\end{array}\right).
\end{align}
To make notation more intuitive we also write $\sigma^2_i = \sigma_{i, i}$. Remember that thresholds are based on the reference set $\mathcal{Y}$, which pools the elements of $\tilde{\mathbf{Y}}_1, \dots, \tilde{\mathbf{Y}}_m$. It can be shown that the expectations of the empirical mean $\bar{Y}$ and variance $S^2$ of the observations in the reference set are
\begin{align}
\mathbb{E}(\bar{Y}) & = \frac{1}{n} \sum_{i = 1}^n \mu_i,
\label{eq:expectation_mu}\\
\mathbb{E}(S^2) & = \frac{m}{mn - 1} \sum_{i = 1}^n (\sigma_{i}^2 + \mu_i^2) \ - \ \frac{1}{n(mn - 1)} \sum_{i = 1}^n \sum_{i' = 1}^n \sigma_{i,i'} \ - \ \frac{m}{n(mn - 1)}\left(\sum_{i = 1}^n \mu_i\right)^2,
\label{eq:expectation_sigma2}
\end{align}
%\begin{align}
%\mathbb{E}(\mean) & = \frac{1}{K} \sum_{i = 1}^K \mu_i,
%\label{eq:expectation_mu}\\
%\mathbb{E}(\sd^2) & = \frac{n}{nK - 1} \sum_{i = 1}^K (\sigma_{i}^2 + \mu_i^2) \ - \ \frac{1}{K(nK - 1)} \sum_{i = 1}^K \sum_{i' = 1}^K \sigma_{i,i'} \ - \ \frac{n}{K(nK - 1)}\left(\sum_{i = 1}^K \mu_i\right)^2,
%\label{eq:expectation_sigma2}
%\end{align}
respectively. Moving to the resulting thresholds, we get
\begin{align}
\mathbb{E}(\hat{q}_{Y, \alpha}) & \approx \mathbb{E}(\bar{Y}) + t_{m\times n - 1, \alpha} \times \left(1 + \frac{1}{m\times n} \right) \times \sqrt{\mathbb{E}(S^2)}
\label{eq:expectation_q}
\end{align}
when using formulation \eqref{eq:q_Y_t}; for the original version \eqref{eq:def_q}, the formula simplifies accordingly. For the thresholds on the original scale
\begin{align}
% \mathbb{E}(\hat{q}_{X, \alpha}) & \approx f^{-1}\left\{\mathbb{E}(\bar{Y}) + t_{m\times n - 1, \alpha} \times \left(1 + \frac{1}{m\times n} \right) \times \sqrt{\mathbb{E}(S^2)}\right\}
\mathbb{E}(\hat{q}_{X, \alpha}) & \approx f^{-1}\left\{\mathbb{E}(\hat{q}_{X, \alpha})\right\}
\label{eq:expectation_q2}
\end{align}
usually holds in good approximation in our setting (i.e., with $f$ the identity function or natural logarithm). Details are provided in Appendix \ref{appendix:derivation_n}.

For $n = 1$, $\bar{Y}$ and $S^2$ are unbiased estimators of $\mu_1$ and $\sigma^2_1$. Under assumptions (HB), (IB) and (N), it is clear that thresholds based on a suitable $t$-distribution (or, for sufficiently large $m$, normal distribution) will be exceeded by a new peak $X_{m + 1}^{(1)}$ with the intended probabilities (see Section \ref{subsec:reformulation}).
% For $n = 1$ and, if using a $t$-distribution, sufficiently large $m$, it can be shown that
% $$
% \mathbb{E}(\hat{q}_{Y, \alpha}) \approx \mu_1 + z_\alpha \sigma_1 = q_{Y, \alpha},
% $$
% where the second equality holds only if the transformed peak values $Y_{j}^{(1)}$ indeed come from a normal distribution (assumption (N)). This is a desirable property as our estimator $\hat{q}_{Y, \alpha}$ of $q_{Y, \alpha}$ is approximately unbiased. In expectation, thresholds on the transformed scale are thus such that the nominal threshold exceedance probabilities (60\%/10\%/2.5\%) are achieved.\todo{re-phrase? Just refer to unbiasedness of $\bar{Y}$, S?} 
However, if a larger value $n > 1$ is chosen, this will generally no longer be the case. Equations \eqref{eq:expectation_mu}--\eqref{eq:expectation_q} tell us by how much $\hat{q}_{Y, \alpha}$ can then be expected to differ from $q_{Y, \alpha}$. By the definition of $\mu_i$ as the expectation of the $i$-th largest observation in a given season, $\mathbb{E}(\bar{Y})$ decreases in $n$. While the expected thresholds also depend on $\mathbf{\Sigma}$, this downward bias will usually translate to the $\hat{q}_{Y, \alpha}$. As a consequence, when choosing $n > 1$, one must expect to classify a larger number of season peaks as high or very high intensity. Intuitively speaking, including observations which are close to peaks, but not actually peaks, dilutes the reference set and pulls thresholds downward. When choosing $n = 30/m$ as suggested for the MEM, thresholds will tend to increase the larger $m$. Consequently, the probability of exceeding thresholds will decrease, approaching the nominal levels from above. 

% Intuitively speaking, if $n = 1$ one observation per season is used, the reference set contains only actual peak values. The set may thus be small, but only contains observations which -- being season peaks themselves -- provide a good reference for future peaks. If $n > 1$, the set becomes larger, but at the cost of a systematic downward bias resulting from the inclusion of non-peak weeks.



\subsection{Smoothing of time series prior to computing thresholds}
\label{subsec:smoothing}

Next we assess the role of smoothing historical data prior to computing thresholds. As derivations get tedious otherwise, we only consider the case where $f$ is the identity function, i.e., no transformation is applied to the reference set and thus $
Y^{(i)}_j = X^{(i)}_j$. Also, as smoothing and multiple observations per season are generally not used in parallel we assume $n = 1$. Now denote by $p_j$ the peak week of the smoothed incidence in season $j$ such that
\begin{equation}
Y_j^{(1)} =  X_j^{(1)} = X^{\text{smo}}_{j, p_j} = \frac{1}{l} \sum_{d = 0}^{l - 1} X^{\text{raw}}_{j, p_j - d}.\label{eq:Y_1_smoothing}
\end{equation}
Moreover, we define a random vector which contains the observations $X^{\text{raw}}_{j, p_j}, \dots, X^{\text{raw}}_{j, p_j - l + 1}$ averaging to $Y_j^{(1)}$ in decreasing order,
$$
\tilde{\mathbf{X}}^\text{raw}_j = \begin{pmatrix} \tilde{X}_j^{\text{raw}, (1)} & = \ \ \max(X^{\text{raw}}_{j, p_j}, \dots, X^{\text{raw}}_{j, p_j - l + 1}) \\ \vdots & \vdots \\ X_j^{\text{raw}, (l)} & = \ \ \min(X^{\text{raw}}_{j, p_j}, \dots, X^{\text{raw}}_{j, p_j - l + 1}) \end{pmatrix}.
$$
Again we maintain assumptions (HB) and (IB) on the independence and homogeneity between seasons, but relax their within-season counterparts (HW) and (IW). The theoretical mean and covariance matrix of $\tilde{\mathbf{X}}^\text{raw}_j$ are denoted by
\begin{align}
\mathbb{E}\left(\tilde{\mathbf{X}}^\text{raw}_j\right) = \boldsymbol{\mu}^\text{raw} = \left(\begin{array}{c}
\mu^\text{raw}_1\\
\vdots\\
\mu^\text{raw}_l
\end{array}\right) \ \ \ \text{and} \ \ \ \text{Cov}\left(\tilde{\mathbf{X}}^\text{raw}_j\right) = \boldsymbol{\Sigma}^\text{raw} =
\left(\begin{array}{ccc}
\sigma^\text{raw}_{1, 1} & \cdots & \sigma^\text{raw}_{1, l}\\
\vdots & \ddots &\vdots\\
\sigma^\text{raw}_{l, 1} & \cdots & \sigma^\text{raw}_{l, l}
\end{array}\right),
\end{align}
respectively. Note that by construction we have $\mu^\text{raw}_1 \geq \mu^\text{raw}_2 \geq \dots > \mu^\text{raw}_l$. As we assumed $n = 1$, the reference set $\mathcal{Y}$ consists simply of $Y^{(1)}_1, \dots, Y^{(1)}_m$ as given in equation \eqref{eq:Y_1_smoothing}. It is straightforward to show that in this case the expectations of $\bar{Y}$ and $S^2$ are
\begin{align}
\mathbb{E}(\bar{Y}) = \frac{1}{l} \sum_{i = 1}^l \mu^\text{raw}_i, \ \ \ 
\mathbb{E}(S^2) = \frac{1}{l^2} \sum_{i = 1}^l \sum_{i' = 1}^l \sigma^\text{raw}_{i', i}.
\label{eq:expectation_moments_smoothing}
\end{align}
These can be plugged into equation \eqref{eq:expectation_q} to approximate the expected thresholds. 

The expressions from \eqref{eq:expectation_moments_smoothing} are obviously unbiased estimates of the mean and variance of the smoothed peak values  $Y_j^{(1)} =  X_j^{(1)} = X^{\text{smo}}_{j, p_j}$. Now assume that normality (N) holds and that thresholds are applied to $Y_{m + 1}^{(1)}$, i.e., a smoothed new peak value. The thresholds based on a $t$-distribution (or, for sufficiently large $m$, a normal distribution) will then have the desired exceedance probabilities. This, however, is not the case if thresholds are applied to unsmoothed new peak values, as done in the WHO method. Calibration is then only achieved if $l = 1$, i.e., if there is no actual smoothing, such that smoothed and unsmoothed peaks agree. For $l > 1$, unsmoothed peaks by construction exceed smoothed peaks, and will thus also exceed thresholds more frequently. % we get by construction $\mathbb{E}(\bar{Y}) \leq \mathbb{E}(Y^{(1)}_{j})$ and usually also $\mathbb{E}(S^2) \leq \text{Var}(Y^{(1)}_{j})$. In practice both are likely to hold in strict inequality. The high and very high thresholds will thus be lower than for $l = 1$, and unsmoothed new peaks will exceed the thresholds more frequently. 
To ensure calibration of thresholds, it is thus necessary to either smooth historical \textit{and} new peaks, or neither of the two. % As in Section \ref{subsec:choice_n}, it is key that the reference set is comparable to new peaks, which is not the case if smoothing is handled inconsistently.

% Similarly to the previous section, for $l = 1$, the estimators of $\bar{Y}$ and $S^2$, and the resulting thresholds will be exceeded by  consequently calibrated thresholds. % and, in case of a $t$-distribution, sufficiently large $m$, we obtain
% \begin{equation}
% \mathbb{E}(\hat{q}_{Y, \alpha}) \approx \mu^\text{raw}_1 + z_\alpha \sigma^\text{raw}_1.
% \label{eq:expected_threshold_smooth}
% \end{equation}
% If in addition the normality assumption (N) holds and $q_{Y, \alpha} = \mu_1^\text{raw} + z_\alpha \sigma_1^\text{raw}$, then equation \eqref{eq:expected_threshold_smooth} implies that the threshold $\hat{q}_\alpha$ is again approximately unbiased. % in the sense that in expectation they are such that the unsmoothed peak of a new season will exceed them with the intended probabilities (60\%/10\%/2.5\%).
% For $l > 1$ we get by construction $\mathbb{E}(\bar{Y}) \leq \mu^\text{raw}_1$ and usually also $\mathbb{E}(\sqrt{S^2}) \leq \sigma^\text{raw}_1$. In practice both are likely to hold in strict inequality. The high and very high thresholds will thus be lower than for $l = 1$ and exceeded more frequently by unsmoothed new peaks. If thresholds are applied to new smoothed peak values, though, this problem is avoided as the mean and variance of the smoothed peak values correspond to the expressions from equation \eqref{eq:expectation_moments_smoothing}.

% As in Section \ref{subsec:choice_n}, the key to ensuring calibrated thresholds is that the reference set need to be comparable to new peaks; both smoothed peak values and values which are close to, but not actually peaks are systematically different from new unsmoothed peak values. They are thus not suitable as reference values, and the resulting thresholds will be biased. While we did not adapt our derivation to the case where data are log-transformed prior to computing thresholds, our simulation results in Section \ref{subsec:sim_smoothing} confirm that the described qualitative patterns likewise hold.



\subsection{Sensitivity and specificity under normality and $n = 1$}
\label{subsec:theory_sensitivity}

While in the previous sections we focused on calibration and expected thresholds, we now turn to the resulting sensitivity and specificity. As discussed in Section \ref{subsec:choice_n}, the assumptions (HW) and (IW) on within-season homogeneity and independence are implausible if $n > 1$. We therefore focus again on the case where $n = 1$ observation is used per historical season and thus $\mathcal{Y} = \{Y_1^{(1)}, Y_2^{(1)}, \dots, Y_m^{(1)}\}$. For the following we require assumptions (HB), (IB) and (N), which in this case amount to
$$
Y_1^{(1)}, \dots, Y_m^{(1)}, Y_{m + 1}^{(1)} \stackrel{\text{i.i.d.}}{\sim} \text{N}(\mu_1, \sigma^2_1).
$$
Defining the threshold $\hat{q}_{Y, \alpha}$ via quantiles of the normal distribution as in equation \eqref{eq:def_q}, it can be shown that
\begin{equation}
\hat{q}_{Y, \alpha} \stackrel{\text{approx}}{\sim} \text{N}\left\{\mu_1 + z_\alpha \sigma_1,\ \ \sigma_1^2 \times \left(\frac{1}{m} + \frac{z_\alpha^2}{2(m - 1)} \right) \right\}.
\label{eq:q_Y}
\end{equation}
For definition \eqref{eq:q_Y_t} based on the $t$-distribution a slightly more involved expression can be obtained, see Appendix \ref{suppl:derivations_sensitivity}. Based on this we can approximate the sensitivity and specificity of our thresholding procedure at level $\alpha$. The sensitivity describes the probability that given the new transformed peak is truly among the $(1 - \alpha) \times 100\%$ highest peaks with respect to the underlying distribution,
$$
Y_{m + 1}^{(1)} \geq q_{Y, \alpha} = \mu_1 + z_\alpha \sigma_1,
$$
it will also be classified as such, i.e.,
$$
Y_{m + 1}^{(1)} \geq \hat{q}_{Y, \alpha}.
$$
It can be shown that the probability in question can be approximated by
\begin{equation}
\text{sens}_\alpha = \text{Pr}\left(Y_{m + 1}^{(1)} > \hat{q}_{Y, \alpha} \ \mid \ Y_{m + 1}^{(1)} > q_{Y, \alpha}\right) \approx \int_{z_\alpha}^\infty \phi(y) \times \Phi\left(\frac{y - z_\alpha}{\sqrt{\frac{1}{m} + \frac{z_\alpha^2}{2(m - 1)}}}\right) \text{d}y,
\label{eq:sens}
\end{equation}
where $\phi$ and $\Phi$ are the standard normal density function and cumulative distribution function, respectively. We note that this expression depends on $\alpha$ and $m$, but not $\mu_1$ and $\sigma^2_1$. Similarly, the specificity can be approximated by
\begin{equation}
\text{spec}_\alpha = \text{Pr}\left(Y_{m + 1}^{(1)} < \hat{q}_\alpha \ \mid \ Y_{m + 1}^{(1)} < q_{Y, \alpha}\right) \approx \int_{-\infty}^{z_\alpha}\phi(y) \times \left\{1 - \Phi\left(\frac{y - z_\alpha}{\sqrt{\frac{1}{m} + \frac{z_\alpha^2}{2(m - 1)}}}\right)\right\} \text{d}y.
\label{eq:spec}
\end{equation}
The positive predictive value can be computed using the formula \citep{Altman1994}
\begin{equation}
\text{PPV}_\alpha = \text{Pr}(Y_{m + 1}^{(1)} > q_{Y, \alpha} \ \mid Y_{m + 1}^{(1)} > \hat{q}_\alpha) = \frac{(1 - \alpha) \times \text{sens}_\alpha}{(1 - \alpha) \times \text{sens}_\alpha + \alpha \times (1 - \text{spec}_\alpha)}.
\label{eq:ppv}
\end{equation}
Expressions \eqref{eq:sens}--\eqref{eq:ppv} have no simpler closed form, but they are straightforward to evaluate numerically. Again, slightly more involved versions of these formulas can be obtained for thresholds based on the $t$-distribution. These are provided in Appendix \ref{suppl:derivations_sensitivity}, along with the derivations.

In Figure \ref{fig:sens_spec_ana} we visualize the sensitivity, specificity and positive predictive values as a function of $m$ and $\alpha = 0.4, 0.9, 0.975 $. As the theoretical approximations may not be very accurate for small $m$, we also show simulation-based versions. For the high and very high thresholds, the $t$-distribution leads to lower sensitivity, but higher specificity and PPV than the normal distribution (which is a consequence of the $t$-distribution being more dispersed). Little surprisingly, the sensitivity, specificity and positive predictive values increase in $m$. The sensitivity and PPV are lowest for the very high threshold at $\alpha = 0.975$. For the most practically relevant values of $5 \leq m \leq 10$, the PPV is only between 0.25 and 0.5 in this case. More than half of the seasons flagged as very high intensity will thus be false positives, i.e., peaks which are not actually among the highest 2.5\%. For the high threshold ($\alpha = 0.9$), the respective PPVs are between 0.5 and 0.7. Even if assumptions (H), (I) and (N) are fulfilled, there are thus natural limits to the classification performance, which for small $m$ and high $\alpha$ are at rather modest levels. We note that this result is the same irrespective of whether any smoothing has been applied (as long as both historical and new peaks are smoothed). While smoothing will usually make thresholds less variable, any effect on sensitivity and specificity is canceled out by the fact that smoothed new peaks are likewise less variable.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale=0.75]{figure/analytical_sens_spec.pdf} \vspace{-5mm}

\includegraphics[scale=0.75]{figure/analytical_sens_spec_t.pdf}
\end{center}

\vspace{-10mm}

\caption{Sensitivity, specificity and positive predictive values under normality assumption, by number $m$ of historical seasons used. Top row: when using the original formulation \eqref{eq:def_q} based on quantiles of the normal distribution. Bottom: when using formulation \eqref{eq:q_Y_t} based on quantiles of the $t$-distribution. Dashed lines show the theoretical approximations \eqref{eq:sens}--\eqref{eq:ppv}, solid lines simulation results.}
\label{fig:sens_spec_ana}
\end{figure}



\subsection{Basing thresholds on confidence intervals}
\label{subsec:cis}

The thresholds from equation \eqref{eq:def_q} have previously been referred to as upper ends of one-sided confidence intervals for the arithmetic (WHO method) or geometric mean (MEM) of the reference observations \citep{WHO2014, Vega2015}. As mentioned in Section \ref{subsec:reformulation}, this is imprecise terminology and the thresholds should instead be interpreted as predictive quantiles. The use of actual confidence interval limits, however, is also possible in the \texttt{mem} package. Equation \eqref{eq:expectation_q} then becomes
$$
\mathbb{E}(q_{Y, \alpha}) \approx \mathbb{E}(\bar{Y}) + \frac{z_\alpha}{\sqrt{nm}} \sqrt{\mathbb{E}(S^2)}.
$$
This is not a desirable property as given enough historical observations (large $m \times n$), thresholds for all levels $\alpha$ will converge to $\mathbb{E}(\bar{Y})$. Each new season peak will then be classified either as low or as very high intensity. % \todo{mention other types of CI?}

\section{Simulation study}
\label{sec:simulation}

\subsection{Setup}
\label{subsec:simulation_setup}

The analytical results from the previous Section involve some approximations and the normality assumption (N) for the observations in the reference set $\mathcal{Y}$. In the following we study the respective aspects empirically in a simulation study. To realistically mimic the seasonal patterns of influenza, we re-sample historical surveillance data rather than generating fully synthetic data. Assume $M$ seasons of historical data on a measure of influenza activity are available. We then repeat the following steps 500 times:

\begin{itemize}
\item[(a)] Sample a sequence of 15 seasons from the $M$ available seasons. This is done with equal probability for each season and \textit{with replacement}, meaning that the same season can appear more than once. This approach is called the \textit{seasonal block bootstrap} \citep{Politis2001}.
\item[(b)] For each value $m = 5, \dots, 15$:
\begin{itemize}
\item[(i)] Restrict the generated sequence to the first $m$ seasons.
\item[(ii)] Compute thresholds for medium, high and very high intensity ($\alpha = 0.4, 0.9, 0.975$). This is done using a number of variations of the thresholding procedure, see below.
\item[(iii)] Evaluate which of the $M$ historical season peaks would be classified as low, moderate, high and very high.
\end{itemize}
\item[(c)] Compute relevant summary statistics including mean thresholds, exceedance probabilities, sensitivities, specificities and positive predictive values. For the three latter, the true intensity levels of the different seasons are determined based on the empirical distribution of the $M$ available season peaks (i.e., the 2.5\% highest peaks are very high intensity, the following 7.5\% high intensity and so on).
\end{itemize}
The range $m =5, \dots, 15$ is motivated by the values found in real-world applications, see Section \ref{sec:review}. We compute thresholds using a total of nine variations of the approach described in Section \ref{sec:definitions}. In a first step, we assess the MEM and WHO method in their current form, i.e. based on quantiles of the normal distribution as in equation \eqref{eq:def_q}.
\begin{enumerate}
\item[(a)] no smoothing, no transformation of the reference set, $n = 1$. This corresponds to the WHO method, but without the optional smoothing step.
\item[(c)] no smoothing, logarithmic transformation of the reference set, $n = 1$.
\item[(b)] no smoothing, no transformation of the reference set, $n = 30/m$.
\item[(d)] no smoothing, logarithmic transformation of the reference set, $n = 30/m$. This corresponds to the default MEM approach.
\end{enumerate}
\noindent To study the impact of smoothing we apply the following settings:
\begin{enumerate}
\item[(e)] smoothing with $l = 3$ (alternatively $l = 7$), log transformation of the reference set, $n = 1$, thresholds applied to unsmoothed new peaks. This corresponds to the WHO method with the optional log transformation applied.
\item[(f)] same as (e), but thresholds applied to smoothed new peaks.
\end{enumerate}
We then address the two extensions of the thresholding procedure proposed in Section \ref{sec:formalization_extension}.
\begin{enumerate}
\item[(g)] no smoothing, logarithmic transformation of the reference set, $n = 1$, using quantiles of a $t$ rather than normal distribution. Thresholds are thus based on equation \eqref{eq:q_Y_t} rather than \eqref{eq:def_q}.
\item[(h)] same as (g), but accounting for a secular trend using eqation \eqref{eq:trend}. To assess this setting, a secular trend is artificially added to the re-sampled sime series, see Section \ref{subsec:results_trends}.
\end{enumerate}
Finally, we illustrate the behaviour if confidence rather than prediction intervals are used.
\begin{enumerate}
\item[(i)] no smoothing, log transformation of the reference set, $n = 1$ (alternatively $n = 30/m$), using confidence rather than prediction intervals.
\end{enumerate}
All analyses were performed using the R language for statistical computing \citep{RCT2020} and the package \texttt{mem} \citep{Lozano2020}. For points (g) and (h), the method was re-implemented independently (see code availability statement at the end of this paper).

\subsection{Data}
\label{subsec:data}

For our re-sampling scheme we use publicly available data on the estimated weekly incidence of influenza-like illness per 100,000 inhabitants in France, 1986--2019, as published by Rseau Sentinelles (INSERM/Sorbonne Universit, \url{https://www.sentiweb.fr}, \citealt{Flahault2006}). These are available both at the national and at the regional level. To make statements about the sensitivity and specificity of the ``very high'' categorization, we require a larger set of historical data than the available 34 seasons. We therefore pool curves from the 12 continental French administrative regions. As the overall level of ILI incidences varies considerably across regions, we scale all data such that the average season peak per region is 100. As the data from the Corse island region differ substantially from the other regions, with overall considerably lower reported incidences, we exclude them. Moreover, all curves from the pandemic 2009/2010 season were removed. In total we then dispose of $12 \times 33 = 396$ historical seasons.

Figure \ref{fig:data} shows an illustration of the re-scaled data from two regions (Grand Est and Nouvelle Aquitaine), along with four descriptive plots. In the bottom left panel we show boxplots of the first through sixth largest observation per season. Not surprisingly, values on average get smaller for increasing ranks. They also get less dispersed, meaning that variability among e.g.\ the sixth largest observations per season is smaller than among the peak values. Values from the same season are strongly correlated across ranks one through six. The next panel shows the distribution of peak values without smoothing ($l = 1$) and with smoothing windows of $l = 3$ and $l = 7$. It is clearly visible that peak values get lower and less dispersed when smoothing is applied. The remaining panels show normal QQ plots of untransformed and log-transformed peak values. It can be seen that after transformation the distribution is roughly normal.

To assess the sensitivity of our simulation results to the choice of data set we re-ran all simulations using  weekly weighted ILI (wILI) data from the US. These data stem from the from CDC \textit{FluView} project (Charbonneau and James, 2019), cover the years 19982018, and were
obtained via the CDC \textit{FluSight} influenza forecasting platform (\url{https://github.com/FluSightNetwork/cdc-flusight-ensemble/}). Reported values are between zero and one and correspond to the fraction of general practitioner visits which are due to influenza-like symptoms. To increase the number of available seasons we pooled national-level data and data from the ten Health and Human Services (HHS) regions, re-scaling data to mean peak values of 100 as described for the French example above. In total we thus obtained $11 \times 19 = 209$ historical seasons. Results based on the US data have been moved to Supplement \ref{suppl:us} and are briefly discussed in Section \ref{subsec:results_us}.

% mention exact definition, source, number of seasons

\begin{figure}[h]
\center
\includegraphics[width=1\textwidth]{figure/plot_data_fr.pdf}
\caption{Top and middle row: Re-scaled estimated numbers of weekly ILI cases in the French regions of Grand Est and Nouvelle Aquitaine, 1985--2019, with the pandemic season 2009/2010 removed. Off-season weeks are omitted, with grey lines delimiting the different seasons. The bottom row shows descriptive plots of the distribution of season peaks. First: Boxplots of incidence values by rank within season. Second: Boxplot of smoothed peak values as a function of the smoothing window width $l$. In these two plots, light grey lines link values stemming from the same season. The numbers above the boxplots indicate the Pearson correlation between the values shown in the first boxplot ($i = 1$ or $l = 1$, respectively) and the boxplot in question. Third: Normal QQ plot of untransformed peak values. Fourth: Normal QQ plot of log-transformed peak values.}
\label{fig:data}
\end{figure}

\subsection{Results based on French data}

\subsubsection{Choice of transformation $f$ and number $n$ of observations used per season}
\label{subsubsec:n}

\textbf{Thresholds and threshold exceedance.} Figure \ref{fig:results1} summarizes thresholds resulting from different combinations of transformation function $f$ and number $n$ of observations used per season (specifications a--d from Section \ref{subsec:simulation_setup}). Note that we here base thresholds on quantiles of a normal distribution as in equation \eqref{eq:def_q}. For each case we show mean thresholds, along with the empirical 5\% and 95\% quantiles, and the shares of new seasons classified into the different categories. This is complemented with summaries of the sensitivity, specificity and positive predictive value. All results are shown as a function of the number $m$ of historical seasons used. Where applicable, the simulation results displayed as squares are complemented by analytical approximations shown as lines. These have been computed using equations \eqref{eq:expectation_q2} and \eqref{eq:sens}--\eqref{eq:ppv} with empirical means and covariances plugged in.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figure/plot_fr.pdf}

\vspace{-1.5cm}

\caption{Impact of the choice of $n$ and transformation function $f$. First column: simulation-based average intensity thresholds (squares) along with bands delimited by the empirical 5\% and 95\% quantiles. Analytical approximations of mean threshold values (computed from empirical means and covariances) are displayed as lines. Second column: resulting average shares of season peaks classified as low, medium, high and very high intensity. Third to fifth columns: sensitivity, specificity and PPVs of the different thresholds. Simulation results are shown as squares. Where available analytical approximations are shown as lines.}
\label{fig:results1}
\end{figure}

Thresholds for high and very high intensity are higher when a log transformation is employed. This leads to better calibration, i.e., the shares of seasons exceeding the high or very high thresholds are closer to the intended levels of 10\% and 2.5\%. Indeed, when using $n = 1$ observation per season and $m \geq 10$ historical seasons, the thresholds based on log-transformed data have close to nominal exceedance rates. Without this transformation, new season peaks are classified as very high in roughly 10\% rather than 2.5\% of the cases. This indicates that the normal assumption is more appropriate after log tansformation, as already visible from the normal QQ plots in Figure \ref{fig:data}.

As implied by the reasoning from Section \ref{subsec:choice_n}, letting the number of observations used per season depend on the number of available seasons via $n = 30/m$ leads to average thresholds which increase in $m$. When using a log transformation, they increase from 180 for $m = 5, n = 6$ to 218 for $m = 10, n = 3$ and 240 for $m = 15, n = 2$. If we choose $n = 1$ irrespective of $m$, as recommended in Section \ref{subsec:choice_n}, the average is around 270. Including historical observations which are not actual peaks thus leads to a considerable lowering of thresholds and increases the number of alerts for high and very high influenza activity. For $m = 5, n = 6$ the proportion of season peaks classified as very high is 15\% if a log transformation is used and 24\% otherwise. As can be seen from the fourth column, this is due to poor specificity, and as shown in the fifth column, leads to low positive predictive values. In the most extreme case where $m = 5$ seasons are used without a transformation, only one in ten seasons peaks classified as very high is actually from the 2.5\% most extreme peaks. We note, however, that even the most well-behaved specification with $n = 1$ and a log transformation only yields a PPV slightly above 20\% for $m = 5$. For $m = 15$ this value roughly doubles. Our theoretical approximation is well aligned with the simulation results here (but much less so if no log transformation is used, as in this case the normality assumption is poorly fulfilled).


It can be noted that the thresholds generally have high variability (see the shaded bands in the first column of Figure \ref{fig:results1}). This is particularly pronounced for the very high threshold, and even more so if a logarithmic transformation is used. This reflects the general difficulty of estimating extreme quantiles from a small number of observations.

\textbf{Confusion matrices.} To complement these results and provide some more intuition on the intensity classifications, Figure \ref{fig:mosaic} shows confusion matrices for thresholds computed with a log transformation and different values of $m$. This represents a more detailed breakup of the results from the second column of Figure \ref{fig:results1}. Note that the perspective differs somewhat from the computation of sensitivities and specificities. There, we focused on \textit{threshold exceedance}, considering for instance whether the medium threshold was exceeded by a peak which was truly at least medium intensity. For the sensitivity of the medium threshold we thus ignored whether a peak which was truly medium intensity also exceeded the high threshold (which was in turn reflected in the specificity of the high threshold). Now we consider not just threshold exceedance, but the exact categorization. This is shown stratified by the true category of a peak, determined via the empirical distribution of all 396 available peaks (the highest 2.5\% are considered very high, the next 7.5\% high, the next 50\% medium and the remaining 40\% low).

For $n = 1$ (top two rows) it can be seen that in all categories misclassifications occur, but become somewhat less frequent for larger $m$. The log transformation leads to better classification of truly medium and high peaks, while for truly low or very high peaks results are better without a transformation. This essentially reflects the fact that thresholds are more spaced out when using a log transformation (see Figure \ref{fig:results1}), giving medium and high peaks a better chance of correct classification. The rather modest positive predictive values from Figure \ref{fig:results1} are here reflected by the fact that in most panels, a large part of the peaks classified as very high (purple rectangles) are actually high or medium.

For $n = 30/m$, the miscalibration issues identified previously are again visible. For $m = 5$ and with a log transformation, more than 60\% of truly low peaks are classified as medium; roughly 40\% of medium peaks are classified as high or very high; and close to 75\% of high peaks are classified as very high. Similar pictures arise for thresholds computed on the natural scale, while as implied by theory, the problem is diminished with increasing $m$.

A natural obstacle to high classification accuracy under the chosen definitions is that e.g., the highest season peaks from the medium category are very similar to the lowest ones from the high category. We illustrate this aspect in Supplementary Figure \ref{fig:mosaic_fancy}, which is a more fine-grained version of Figure \ref{fig:mosaic}. Here we display the classification proportions as a function of the true quantile level a season peak corresponds to. These proportions change smoothly, and as one would expect, the chance of misclassification is particularly high for peaks close to the 40th, 90th and 97.5th percentiles of the underlying distribution.

\begin{figure}
\includegraphics[width=0.9\textwidth]{figure/mosaic_fr.pdf}
\caption{Confusion matrices for intensity classifications obtained with different choices of $n$ and transformation function $f$. Mosaic plots show which fractions of season peaks which are truly very high, high, medium or low are classified into the four categories. The true class is determined with respect to the empirical quantiles of the distribution of peaks: very high (highest 2.5\% of all peaks), high (next 7.5\%), medium (next 50\%), low (lowest 40\% of all peaks).}
\label{fig:mosaic}
\end{figure}

% When always using $n = 1$, the average thresholds and shares of the different categories are more well-behaved also for small $m$. This holds especially when applying a log transformation, even though certain mismatches with the nominal exceedance probabilities remain. Also, there is considerable variability in the estimated thresholds (shaded areas in Figures \ref{fig:results1} and \ref{fig:results2}). These difficulties, however, are inherent in the problem of estimating a 90\% or 97.5\% quantile based on 5--10 observations.

% The increase of thresholds obtained with $n = 30/m$ as more historical data accumulate is not only a statistical tendency. For all four countries, the probability that the threshold for high intensity increases when using the first ten rather than only the first five years of a sampled sequence was around 90\%. For illustration of this aspect, consider the following example: We repeat a sequence of five ILI seasons from France (2014--2019) four times to obtain a time series of twenty seasons. We then apply the moving epidemic method with log transformation and $n = 30/m$ using the first $m = 5, 6, \dots, 20$ years of data, mimicking the development of thresholds as more historical data become available. As can be seen from the top panel of Figure \ref{fig:example_france}, this results in gradually increasing thresholds, despite the fact that the data in each five-year block behave the same. For example, season 6 is classified as very high intensity, but seasons 11 and 16 are not, even though they are identical to season 6 and the respective data used to construct thresholds follow exactly the same patterns. The bottom panel shows results obtained for the same time series with $n = 1$. While thresholds fluctuate quite a bit (with fluctuations dampening as the time series gets longer), there is no systematic trend.



%\begin{figure}
%\center
%\includegraphics[scale=0.6]{figure/example_france.pdf}
%\caption{Toy example to illustrate increasing thresholds as historical data accumulates: Data from France, 2014/2015--2018/2019 are appended three times and thresholds are computed using $m = 5, \dots, 15$ years of training data. The thresholds based on years 1 through $m$ are overlaid with the incidence time series from the $(m + 1)$-th year.}
%\label{fig:example_france}
%\end{figure}

\subsubsection{Smoothing of time series prior to computing thresholds}
\label{subsec:sim_smoothing}

\textbf{Thresholds and threshold exceedance.} We next assess the impact of smoothing historical data prior to computing thresholds (specifications e--f from Section \ref{subsec:simulation_setup}). Results for a window size of $l = 3$ and including a log transformation are shown in Figure \ref{fig:results_smoothing}. The top row shows the case where thresholds are applied to unsmoothed new peak values. In accordance with the theoretical arguments from Section \ref{subsec:smoothing}, the high and very high thresholds are exceeded more frequently than intended. When applying thresholds to smoothed new observations, the empirical and nominal exceedance levels are in better agreement. We note that thresholds are somewhat less variable than without smoothing, which we consider a desirable feature. As predicted by theory, sensitivity, specificity and positive predictive values are largely the same as without smoothing (compare the second rows of Figures \ref{fig:results1} and \ref{fig:results_smoothing}). While they are slightly higher in the present case we are unsure whether this will be the case in general. We provide a display for a stronger smoothing with $l = 7$ in Supplementary Figure \ref{fig:results_smoothing7}. If the resulting thresholds are applied to unsmoothed new peaks, more than a fifth of them is classified as very high intensity.

\begin{figure}[h!]
\centering
\includegraphics[width=0.95\textwidth]{figure/plot_smoothing3_fr_small.pdf}

\caption{Impact of smoothing of historical data on thresholds. We applied a moving average with $l = 3$ to the historical time series prior to computing thresholds and subsequently applied them to either unsmoothed or smoothed new peak values. Results are shown for thresholds computed with a log transformation. See the caption of Figure \ref{fig:results1} for details on the plot elements.}
\label{fig:results_smoothing}
\end{figure}


\textbf{Confusion matrices.} We complement this again with confusion matrices, shown in Supplementary Figure \ref{fig:mosaic_smoothing}. When unsmoothed new peaks are classified based on thresholds computed with smoothing, peaks tend to be assigned to too high categories. When thresholds are applied to smoothed new peaks, the results strongly resemble those obtained without smoothing of either historical or new peaks (compare second row of Figure \ref{fig:mosaic}).


\subsubsection{Employing the $t$-distribution}

\textbf{Thresholds and threshold exceedance.} In Figures \ref{fig:results1} and \ref{fig:results_smoothing}, it is evident that even when using $n = 1$, the thresholds are somewhat miscalibrated for small $m$. As argued in Sections \ref{subsec:reformulation} and \ref{subsec:normal_vs_t}, in this case it is necessary to base thresholds on a $t$ rather than a normal distribution. Indeed, as visible in Figure \ref{fig:t}, this leads to close-to-nominal exceedance fractions across all considered values of $m$ when employing a log transformation. For thresholds computed on the natural scale, the exceedance fractions remain somewhat miscalibrated. A characteristic of these thresholds which may seem surprising is that the average values of the very high and high thresholds decrease in $m$. This is due to the somewhat counterintuitive fact that well-calibrated predictive quantiles are generally not unbiased estimates of the respective theoretical quantiles. How biased the predictive quantiles need to be in order to remain calibrated depends on their variability and thus $m$.

\textbf{Confusion matrices.} Confusion matrizes for this setting are provided in Supplementary Figure \ref{fig:mosaic_t}. In comparison to Figure \ref{fig:mosaic}, fewer seasons are categorized as high or very high intensity, but the fact that mis-classifications are very common remains qualitatively unchanged.

\begin{figure}[h!]
\begin{center}
\includegraphics[width = \textwidth]{figure/plot_t_fr_small.pdf}
\end{center}
\caption{Summary of simulation results when thresholds are based on $t$ rather than normal distributions. See caption of Figure \ref{fig:results1} for details on the plot elements.}
\label{fig:t}
\end{figure}

\subsubsection{Accounting for secular trends}
\label{subsec:results_trends}

\textbf{Adapted simulation setup.} To assess thresholds which account for secular trends we modify our simulation setting and artificially introduce such trends. To mimic an annual growth rate $r$, we multiply the values $x_{j, k}$ for seasons $j = 1, \dots, m + 1$ by $(1 + r)^{-(m + 1 - j)}$, respectively. The $m + 1$-th season remains unaffected by this modification and still follows the same distribution as in the previous sections. The seasons $1, \dots, m$ are characterized by a geometric trend approaching the usual level from above or below. For our simulation we consider $r = \pm 3\%$ and $\pm 7\%$, computing thresholds with and without a correction for secular trends. In all cases we use a logarithmic transformation and $n = 1$ along with a $t$-distribution.

\noindent \textbf{Thresholds and threshold exceedance.} Figure \ref{fig:trend} shows that ignoring secular trends leads to mis-calibrated thresholds, with stronger miscalibration for larger $m$. This is expected as seasons far in the past in expectation differ more and more strongly from the upcoming $(m + 1)$-th one. Thresholds corrected for secular trends using model \eqref{eq:trend} are wel-calibrated. However, due to the addition of a parameter, the threshold values get considerably more variable. The same display for $r = \pm 7\%$ is available in Supplementary Figure \ref{fig:trend7} and shows the same qualitative patterns.

\begin{figure}
\begin{center}
\includegraphics[width = 0.9\textwidth]{figure/plot_trend3_fr_small.pdf}
\end{center}
\caption{Average thresholds and exceedance shares in the presence of constant annual growth (3\%) and decrease (-3\%). In each setting we computed thresholds with and without accounting for the secular trend.}
\label{fig:trend}
\end{figure}

The sensitivity, specificity and positive predictive values of models with and without secular trends in different settings are compared in Supplementary Figure \ref{fig:cost_trend}. If there is no true secular trend ($r = 0$), the inclusion of a superfluous trend parameter substantially decreases all three performance metrics, most strongly for small $m$. For the considered values of $m$, five to ten additional observations seem to be necessary to even out the cost of increased complexity. As suspected by \cite{Vega2013}, if secular trends are present but not accounted for, the performance of thresholds can decrease from a certain value of $m$ onwards. Little surprisingly, sensitivity will be compromised in case of downward trends, while upward trends hamper specificity and PPVs.

\noindent \textbf{When is it beneficial to correct thresholds for secular trends?} In many cases (consider e.g., PPVs for $r = 3\%$), accounting for trends in thresholds only pays off from a certain value of $m$ onwards. However, it is difficult to provide general guidance on when to trade the bias resulting from ignoring a secular trend for the increased estimation variability resulting from an additional parameter. A heuristic recommendation we can draw from the graphic displays in Figure \ref{fig:trend} and Supplementary Figure \ref{fig:trend7} is that thresholds accounting for trends are extremely variable for $m < 10$. As discussed before, such high variability also implies that the high and very high thresholds need to be higher in expectation in order to be calibrated. This pattern which is very pronounced in our examples. From $m = 10$, or better $m = 15$ onwards, thresholds are better-behaved. This finding should be taken into account when deciding on the inclusion of a trend parameter, see also the Discussion section.


\subsubsection{Basing thresholds on confidence intervals}

Lastly we address thresholds based on confidence intervals (specifications g--h from Section \ref{subsec:simulation_setup}). As mentioned in Section \ref{subsec:cis} these are not the default in any of the  considered methods. However, they are implemented in the \texttt{mem} R package and due to terminological imprecisions in previous methods descriptions may be used by analysts programming their own routines. Figure \ref{fig:cis} shows that these thresholds are not well-behaved. As our theoretical reasoning suggested, the mean thresholds at different levels approach each other as $m$ grows. Overall, a considerably too large number of seasons is classified as very high intensity.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.9\textwidth]{figure/plot_ci_fr.pdf}
\end{center}
\caption{Average thresholds and exceedance shares when thresholds are based on confidence intervals rather than prediction intervals. See the caption and legend of Figure \ref{fig:results1} for details on the plot elements.}
\label{fig:cis}
\end{figure}




\subsection{Short summary of results based on US data}
\label{subsec:results_us}

The results obtained using the US data, shown in Supplementary Section \ref{suppl:us}, are overall in good agreement with those presented for France. While there are some differences in the exact values of exceedance probabilities, sensitivities etc., the overall patterns and conclusions are identical. The agreement with the analytical approximations tends to be somewhat better for the US data. % We take this as a sign of robustness of our findings.

% \textbf{Need to include t stuff and others}

\section{Discussion}
\label{sec:discussion}

\subsection{Practical recommendation}

We provided a statistical assessment of implementation choices in a widely used framework for the computation of influenza intensity thresholds. Our practical recommendation in light of the theoretical and empirical findings is as follows. We suggest including only $n = 1$ observation per season into the reference set, employing a log transformation and basing thresholds on a $t$ rather than a normal distribution. To make thresholds somewhat less variable, data can be smoothed using a moving average with $l = 3$. If smoothing is employed, the resulting thresholds should be applied to new data smoothed using the same procedure. Given the cost of estimating an extra parameter from limited data, our preliminary recommendation is that secular trends should only be accounted for if (a) this is supported by convincing epidemiological reasons and (b) a relatively long historical time series of at least $m = 10$ to 15 is available. % We found that log transforming incidences prior to the computation of thresholds led to better behaved thresholds and closer to nominal exceedance rates. Concerning the question of how many observations per historical season should be included to compute thresholds, we found that the common choice $n = 30/m$ results in too low thresholds when few historical seasons are available. We therefore recommend adopting $n = 1$ irrespective of the number of available historical seasons. This is possible in the \texttt{R} package \texttt{mem} by setting \texttt{i.n.max = 1} when the function \texttt{memmodel} is called. Smoothing historical data prior to the computation of thresholds leads to somewhat less variable thresholds. However, it is important that the same smoothing procedure is also applied to new data before classifying the peak intensity. Otherwise a too large number of thresholds is classified as high or very high. We discourage the use of thresholds based on confidence intervals as they lead to an excessive number of seasons being classified as ``very high''.


\subsection{Conclusions in terms of desirable properties from Section \ref{sec:desirable_properties}}

For a more detailed conclusion we return to the criteria evoked in Section \ref{sec:desirable_properties}.

\begin{description}
\item \textbf{Calibration.} The key to well-calibrated thresholds is comparability of the reference set and new season peaks. This implies that no historical non-peak values should be included (i.e., one should choose $n = 1$). Otherwise thresholds will be pulled downwards and exceeded too frequently by new peaks. If historical data are smoothed ($l > 1$), comparability should be ensured by applying the same smoothing to new season peaks. Otherwise thresholds will again be exceeded too often. If the reference set only contains few values, thresholds should be based on quantiles of a $t$ rather than a normal distribution. Empirically we found that thresholds computed on log-transformed observations were better calibrated. We expect this to translate to other settings as distributions of peak values are typically skewed.\\
Discussions with public health experts showed that, based on practical use over the years, at least some were aware of the the miscalibration of the MEM and WHO methods. These analysts took the bias towards higher categories into account in their interpretation and overall considered it less problematic than an insensitive, anti-conservative system.
\item \textbf{Sensitivity and specificity.} Our theoretical and empirical results show that even under ideal conditions, the sensitivity and positive predictive values of thresholds cannot be expected to exceed rather modest levels if $m$ is small. This aspect is unaffected by whether smoothing is applied and reflects the general difficulty of estimating extreme quantiles from few observations. Especially for the very high threshold at $\alpha = 0.975$, sensitivity and PPV must be expected to be modest and it can be asked whether these thresholds are practically meaningful. In any case it does not seem advisable to add even more extreme thresholds to the procedure. This problem is reduced if many historical seasons are available, but in practice there may be a tradeoff with the recency and comparability of these data. Secular trends in the data can be accounted for, but the addition of a parameter makes estimation less efficient. %  Unless a lot of historical seasons are available or trends are very pronounced their inclusion may not be advisable. % If there are relelvant temporal trends or changes in surveillance systems, historical data may get outdated and their inclusion may not be helpful.
\item \textbf{Stability.} The same difficulties limiting the sensitivity and PPV imply that thresholds must be expected to be rather variable if $m$ is small. Empirically we found that this is even more the case if a log transformation is employed and if secular trends are accounted for. Smoothing of historical data helps to somewhat reduce the variability of thresholds, but without improving sensitivity or specificity.
\item \textbf{Simplicity.} From a statistical standpoint the MEM and WHO methods are straightforward. However, exchange with users from public health indicates that given the large number of possible configurations, both methods are perceived as complex. Our subjective opinion is that the methods overall strike a good balance here. However, more specific guidance on how to choose the parameters of the methods and sensible default values is important to enable successful use of the methods. We note that a somewhat more sophisticated approach based on generalized additive models (GAMs) has recently been suggested \citep{Pang2023}.
\item \textbf{Ease of practical application.} Both methods can be applied using graphical user interfaces and thus do not require programming knowledge. For the MEM this is set up on top of an open-source R package with excellent documentation. For more technically versed users we recommend using this package directly. Combined with minimal data pre-processing it covers all aspects discussed in the present paper and enhances the automation and reproducibility of computations. Unfortunately, the use of a $t$-distribution is currently not implemented, but we provide a simple implementation in R (see code availability statement).
\end{description}

\subsection{Further conclusions and outlook}


% Our theoretical and empirical results highlight the general difficulty of estimating extreme quantiles of distributions from few historical observations. Even if the underlying assumptions are fulfilled, thresholds at $\alpha = 0.975$ will have modest positive predictive values. Inclusion of more historical seasons may help, but there is a tradeoff with the recency and comparability of these data. If there are relelvant temporal trends or changes in surveillance systems, historical data may get outdated and their inclusion may not be helpful. In light of these difficulties it can be asked whether the thresholds for very high incidence are practically meaningful. In any case it does not seem advisable to add even more extreme thresholds to the procedure.

As evoked in the introduction, intensity thresholds are not only used to classify season peaks retrospectively, but also to assess intensity in real time in a weekly rhythm. In this setting, the MEM and WHO thresholds may not be very informative as they are designed for peak incidences and will rarely be exceeded early on in a season. Thresholds which, following the idea of the Serfling method, are specific to calendar weeks or weeks since season onset may be more suitable. Indeed, such an approach has been used to derive intensity thresholds for deaths due to pneumonia and influenza in \cite{Biggerstaff2017}. In discussions with public health experts, however, it was remarked that due to the variable seasonal timing of influenza, the underlying idea of ``average seasonal shape'' may not be adequate for all indicators. % This reflects that the choice of appropriate indicators is a public health rather than purely statistical question, and requires further interdisciplinary exchange. % Thresholds specific to each calendar week could also be defined (as is done e.g., in the Serfling method, see \citealt{Serfling1963}). Their interpretation, however, may be hampered by the strong variability in influenza season timing.

In this manuscript we focused on the assessment of season peak values, but this could be complemented with other indicators like the season total. We note that by thresholding smoothed incidence time series, we actually assess ``peak periods'', which are a middle ground between peak intensity and season totals. Similarly, one could re-define the thresholding procedure such that thresholds are not only applied to peak values, but the $m$ highest values of a season. In such a setting, the default MEM thresholds with $m > 0$ could be expected to be well-calibrated. However, then the question arises how the pattern of exceedance and non-exceedance for the $m$ highest values should be translated to an overall season assessment; we therefore consider the smoothing approach a more practical choice. \todo{cite Green and Punk on ``all observation'' reference sets}

Technically it is straightforward to adjust thresholds for secular trends, and in our discussions, public health experts considered it desirable to do so. However, it is unclear under which circumstances it is statistically advisable to include a trend parameter. In lack of a practical statistical criterion, our preliminary recommendation is to include a secular trend if epidemiological background knowledge supports this choice, but otherwise stick with a more parsimonious model excluding a trend. If an exceptionally long historical time series such as in the French example is available, a trend parameter can be more easily incorporated. In our simulation study, it appeared that from $m = 10$ to 15 onward, thresholds accounting for trends become better-behaved, but it is difficult to provide a general rule.

A related, but even more challenging question is how to address structural breaks in surveillance time series, as recently caused by the COVID-19 pandemic. A regression-based approach similar to \eqref{eq:trend} could be used to account for a shift in overall influenza levels. However, this would nonetheless require several years of post-pandemic data, and it is unclear if the implicit assumption of constant variance $\sigma^2_\varepsilon$ is justified. Also, it is unclear how extreme seasons in the historical data should be handled. These can have a substantial effect on thresholds. In our simulation study we pragmatically removed observations from the pandemic season 2009/2010, as was done in \cite{Vega2015}. However, a more principled approach to determining which historical seasons should be excluded would be desirable. As the same problem arises in outbreak detection, this strand of literature may be a helpful starting point (see e.g., \citealt{Noufaily2013}).


On a more general note, it can be asked whether a categorization into four categories is the most appropriate way of conveying influenza activity. As evoked in Section \ref{subsubsec:n}, the highest season peaks from one category and the lowest from the one above are not qualitatively different, and a discrete categorization may seem somewhat arbitrary. An alternative is to report the percentile of the fitted distribution corresponding to a new peak. This would result in a continuous scale from 0 to 100, with higher values indicating higher intensity. However, given the discussed difficulties with estimating extreme quantiles, a percentile-based display may also convey a false sense of exactness. In discussions with users from public health, it was moreover pointed out that the purpose of intensity classification was precisely to make high-level statements which are easy to communicate to decision makers and the public. In practice, a continous rating would again need to be translated to an appropriately named category, with the same issues as discussed above.

% In our two empirical examples we found the normal distribution to provide a quite good fit to the distribution of log-transformed season peak incidences. However, it is unclear how generalizable this is. For indicators which represent fractions, like positivity percentages, it may not be a suitable choice as in principle the threshold for high or very high activity could be above 100\%. Then different transformations like the logit or different distributional assumptions would be necessary. For the US ILI percentages, no practical problem arose, but this may well be the case for other indicators.




A limitation of our simulation study is that the re-sampled data do not actually stem from one and the same time series, but from several different regions. This made some re-scaling necessary. A strength of the re-sampling scheme is that it ensures a realistic correlation structure between season peaks and the surrounding values, which would be challenging to achieve with fully synthetic data. A weakness is that dependence structures across seasons are not preserved. We re-ran all simulation studies using a smaller data set from the United States and obtained very similar results. We take this as a sign of robustness at least for temperate settings. Intensity thresholds in tropical regions pose specific challenges like seasons with multiple peaks, which we did not address in the present manuscript.



% We focused on the computation of influenza intensity thresholds, but the same general arguments apply to the computation of thresholds to determine the onset of a season \citep{Vega2012}. As variability among observations around the season onset may be lower than around the peak the problem may be less severe; we did not assess this empirically as not all our data sets fully covered the off-season.

% \textbf{Challenge: re-calibration past COVID; estimation with little data; sharing across states? Add Matt points} 

To conclude, we re-emphasize the importance of a simple and interpretable thresholding method with a thorough open source software implementation like \texttt{mem}. The use of a standard approach will improve comparability of results and facilitate further methodological advances. With this work we hope to contribute to the development of best practices  with a statistical perspective, complementing public health practitioners' applied experience.

% Discuss double use, whether Serfling is more appropriate for real-time monitoring (additional challenge: irregular seasonality; maybe suggest weeks since onset) and use Juul argument that week-wise pointwise bands understate the likely height of peak.

\section*{Data and code}

Materials to reproduce the presented results are available at \url{https://github.com/jbracher/mem_who}. A stand-alone R implementations of thresholds based on the $t$-distribution and accounting for secular trends is available in the file \url{TO ADD}. 

\section*{Ethics statement}

No ethics approval was necessary as this study uses exclusively publicly available data.

\section*{Copyright information for Figure \ref{fig:maps}}

Top panel: Materials developed by CDC and available in the public domain (\url{https://www.cdc.gov/other/agencymaterials.html}). The use of this material in the present paper does not imply any form of endorsement by CDC, ATSDR, HHS or the United States Government. The materials are available free of charge from the CDC website. Bottom panel: Copyright by the World Health Organization, 2023, and European Centre for Disease Prevention and Control, 2023. Reproduction is authorised, provided the source is acknowledged. Map data: copyright by EuroGeographics and UN-FAO. 

% \section*{Author contributions}

% Study concept: JB; Data processing: JB and JL; Literature review: JB and JL; Statistical analysis / implementation: JB; Writing and editing: JB and JL.


\section*{Acknowledgements}

We would like to thank Sebastian Funk, Michael H\"ohle, Rob Moss, Laura Werlen and Daniel Wolffram for helpful discussions. Johannes Bracher was supported by the Helmholtz Foundation via the SIMCARD Information and Data Science Pilot Project as well as Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) -- project number 512483310.

{\footnotesize
\bibliographystyle{apalike}
\bibliography{bibliography_mem}
}

\newpage
\appendix


\renewcommand{\thepage}{S\arabic{page}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
% reset counters
\setcounter{page}{1}
\setcounter{section}{0}
\setcounter{table}{0}
\setcounter{figure}{0}

\begin{center}
{\LARGE Supplementary Material: \textit{A statistical assessment of influenza intensity thresholds from the moving epidemic and WHO methods}}

\medskip

{\large Johannes Bracher and Jonas M. Littek}
\end{center}



\section{Mathematical details, derivations of analytical approximations}
\label{appendix:derivations}

\subsection{Details on Section \ref{subsec:regression}}
\label{suppl:regression}

We consider a classic multiple linear regression model (e.g., \citealt{Fahrmeir2013})
$$
\mathbf{Y} = \mathbf{Z}\boldsymbol{\beta} + \boldsymbol{\varepsilon},
$$
where $\mathbf{Y}, \boldsymbol{\varepsilon} \in \mathbb{R}^{m\times 1}, \mathbf{Z}\in \mathbb{R}^{m \times k}$ and $\boldsymbol{\beta} \in \mathbb{R}^{k \times 1}$. Additionally we assume normality of the error terms,
$$
\boldsymbol{\varepsilon} \sim \text{N}(\boldsymbol{0}, \sigma^2_\varepsilon \mathbf{I}_m).
$$
The parameter estimates $\hat{\boldsymbol{\beta}}$ can be estimated using ordinary least squares (OLS). The error variance is estimated via
$$
\hat{\sigma}^2_\varepsilon = \frac{1}{m - k} (\mathbf{Y} - \mathbf{Z}\hat{\boldsymbol{\beta}})^\top (\mathbf{Y} - \mathbf{Z}\hat{\boldsymbol{\beta}}).
$$
The predictive $\alpha$ quantile for a new observation with covariate vector $\mathbf{Z}_0$ is
\begin{equation}
\mathbf{z}_0^\top\hat{\boldsymbol{\beta}} + t_{n - p, \alpha} \times \hat{\sigma}_\varepsilon\times \sqrt{1 + \mathbf{z}_0^\top (\mathbf{Z}^\top\mathbf{Z})^{-1} \mathbf{z}_0}.\label{eq:quantile_general}
\end{equation}
In our ``intercept plus trend'' model \eqref{eq:trend}, we simply have
$$
\mathbf{Z} = \left( \begin{array}{cc}
1 & 1\\
1 & 2\\
\vdots & \vdots\\
1 & m
\end{array}\right),\ \ \ \mathbf{z}_0 = \left( \begin{array}{cc}
1 & m + 1
\end{array}\right), \ \ 
\boldsymbol{\beta} = \left( \begin{array}{c}
\beta_0 \\ \beta_1
\end{array}\right).
$$
Simple but somewhat lengthy computations then imply that in this special case \eqref{eq:quantile_general} becomes
\begin{equation}
\hat{\beta}_0 + \hat{\beta}_1 \times (m + 1) + t_{n - p, \alpha} \times \hat{\sigma}_\varepsilon\times \sqrt{1 + \frac{2\times(2m + 1)}{m\times (m - 1)}}
\end{equation}
where
$$
\hat{\sigma}^2_\varepsilon = \frac{1}{m - 2} \times \sum_{j = 1}^m \left(Y^{(1)}_j - \beta_0 - \beta_1 \times j\right)^2.
$$

\subsection{Derivations for Section \ref{subsec:choice_n}}
\label{appendix:derivation_n}

We start by addressing the expectations of empirical mean $\bar{Y}$ and variance $S^2$ of the reference observations, where
\begin{align*}
\bar{Y} & = \frac{1}{m \times n} \sum_{j = 1}^m \sum_{i = 1}^n Y_i^{(j)}\\
S^2 & = \frac{1}{m\times n - 1} \sum_{j = 1}^m \sum_{i = 1}^n \left(Y_i^{(j)} - \bar{Y}\right)^2.
\end{align*}
It is straightforward to see that 
\begin{equation}
\mathbb{E}(\bar{Y}) = \frac{1}{n} \sum_{i = 1}^n \mu_i.
\end{equation}
For the variance $S^2$, we first note that it can be re-written as
\begin{align}
S^2 & = \frac{m\times n}{m\times n - 1} \left\{\frac{1}{m\times n} \sum_{j = 1}^m \sum_{i = 1}^n \left(Y_i^{(j)} - \bar{Y}\right)^2 \right\}\\
& = \frac{m\times n}{m\times n - 1} \left\{ \underbrace{\frac{1}{m\times n} \sum_{j = 1}^m \sum_{i = 1}^n Y_i^{(j)2}}_{\text{denote this by } a} \ \ \ - \ \ \ \underbrace{\left(\frac{1}{m\times n} \sum_{j = 1}^m \sum_{i = 1}^n Y_i^{(j)} \right)^2}_{\text{denote this by } b} \right\} \label{eq:sigma2hat}
\end{align}
%$$
%\sd^2 = \underbrace{\left(\frac{1}{nK - 1} \sum_{j = 1}^n \sum_{i = 1}^K Y_i^{(j)2}\right)}_{ = a} \ \ \ - \ \ \ \underbrace{\left(\frac{1}{nK - 1} \sum_{j = 1}^n \sum_{i = 1}^K Y_i^{(j)} \right)^2}_{= b}.
%$$
We consider the two terms $a$ and $b$ separately, starting by
\begin{align*}
\mathbb{E}(a) & = \frac{1}{m\times n} \sum_{j = 1}^m \sum_{i = 1}^n \mathbb{E}\left(Y_i^{(j)2}\right)\\
& = \frac{1}{m\times n} \sum_{j = 1}^m \sum_{i = 1}^n \left\{ \text{Var}\left(Y_i^{(j)}\right) + \mathbb{E}\left(Y_i^{(j)}\right)^2 \right\}\\
& = \frac{m}{m\times n} \sum_{i = 1}^n (\sigma_{i}^2 + \mu_i^2).
\end{align*}
Then we note that
\begin{align*}
\mathbb{E}(b) & = \mathbb{E}\left\{\left(\frac{1}{m\times n} \sum_{j = 1}^m \sum_{i = 1}^n Y_i^{(j)} \right)^2\right\}\\
& = \text{Var}\left( \frac{1}{m \times n} \sum_{j = 1}^m \sum_{i = 1}^n Y_i^{(j)} \right) \ \ \ + \ \ \ \mathbb{E}\left(\frac{1}{m \times n}  \sum_{j = 1}^m \sum_{i = 1}^n Y_i^{(j)} \right)^2\\
& = \frac{1}{(m \times n)^2}\sum_{j = 1}^m \text{Var}\left(\sum_{i = 1}^n Y_i^{(j)} \right) \ \ \ + \ \ \ \left(\frac{m}{m \times n} \sum_{i = 1}^n \mu_i\right)^2\\
& = \frac{m}{(m \times n)^2} \sum_{i = 1}^n \sum_{i' = 1}^n \sigma_{i,i'} \ \ \ + \ \ \ \frac{m^2}{(m \times n)^2}\left(\sum_{i = 1}^n \mu_i\right)^2.
\end{align*}
Plugging these results back into equation \eqref{eq:sigma2hat} we obtain
\begin{equation}
\mathbb{E}(S^2) = \frac{m}{m \times n - 1} \sum_{i = 1}^n (\sigma_{i}^2 + \mu_i^2) \ \ \ - \ \ \ \frac{1}{n(m \times n - 1)} \sum_{i = 1}^n \sum_{i' = 1}^n \sigma_{i,i'} \ \ \ - \ \ \ \frac{m}{n(m \times n - 1)}\left(\sum_{i = 1}^n \mu_i\right)^2.
\label{eq:expectation_sigma2_2}
\end{equation}
It is straightforward to see that for $n = 1$ the expressions \eqref{eq:expectation_mu} and \eqref{eq:expectation_sigma2_2} simplify to
$$
\mathbb{E}(\bar{Y}) = \mu_1 \ \ \text{ and } \ \ \mathbb{E}(S^2) = \sigma^2_1,
$$
as given in equation \eqref{eq:expectation_sigma2}.

There is no general way of computing the expectation $\mathbb{E}(S)$ from $\mathbb{E}(S^2)$, but unless the true distribution of  $S^2$ has strong excess curtosis,
\begin{equation}
\mathbb{E}(S) \approx \sqrt{\mathbb{E}(S^2)}
\label{eq:expectation_sigma}
\end{equation}
is a reasonable approximation. We can then plug equations \eqref{eq:expectation_mu} and \eqref{eq:expectation_sigma} into the formulae for the thresholds $q_{Y, \alpha}$ on the transformed scale and obtain
$$
\mathbb{E}(\hat{q}_{Y, \alpha}) \approx \mathbb{E}(\bar{Y}) + z_\alpha \sqrt{\mathbb{E}(S^2)},
$$
where $z_\alpha$ is the $\alpha$ quantile of the standard normal distribution (with $\alpha \in \{0.4, 0.9, 0.975\}$).

If $f$ was set to the natural logarithm, the question remains how to obtain statements concerning thresholds $\hat{q}_{X, \alpha}$ on the original scale. Approximation via a second-order Taylor expansion yields
\begin{equation}
\mathbb{E}(\hat{q}_{X, \alpha}) = \mathbb{E}\left\{\exp(\hat{q}_{Y, \alpha})\right\} \approx \exp\left\{\mathbb{E}(\hat{q}_{Y, \alpha})\right\} \times \left\{1 + \frac{\text{Var}(\hat{q}_{Y, \alpha})}{2} \right\}.
\label{eq:taylor}
\end{equation}
Empirically, after transformation to the log scale, the variance of the reference observations is low in our applied setting. The resulting variances of $q_{Y, \alpha}$ are then quite small and do not play an important role in equation \eqref{eq:taylor}. We can thus use the even simpler approximation
\begin{equation}
\mathbb{E}(\hat{q}_{\hat{Y}, \alpha}) \approx \exp\left\{\mathbb{E}(\hat{q}_{Y, \alpha})\right\} \approx \exp\{\mathbb{E}(\bar{Y}) + z_\alpha \sqrt{\mathbb{E}(S^2)}\}.
\end{equation}
As can be seen from Figures 2 and 3 from the main manuscript, this approximation works very well in practice. % To compute the values indicated by the small black crosses, we just plugged the empirical mean vectors and covariance matrices of the $\mathbf{Y}_j$ in the respective data sets into equations \eqref{eq:expectation_mu}--\eqref{eq:expectation_q}.

\subsection{Derivations for Section \ref{subsec:smoothing}}

Remember that we set $f$ to the identity function, denote by $p_j$ the peak week of the smoothed incidences in season $j$ and define
$$
Y_j^{(1)} = X_j^{(1)} = X^\text{smo}_{j, p_j} = \sum_{d = 0}^l X^\text{raw}_{j, p_j - d} = \begin{pmatrix} 1/l \\ \vdots \\ 1/l \end{pmatrix}^\top \tilde{\mathbf{X}}^{\text{raw}}_j.
$$
As we assume that $n = 1$, equation \eqref{eq:moments} simplifies to
\begin{align}
\begin{split}
\bar{Y} & = \frac{\sum_{j = 1}^m Y_j^{(1)}}{nm}\\
S^2 & = \sqrt{ \frac{\sum_{j = 1}^m (Y_j^{(1)}  - \bar{Y})}{nm - 1}}.
\end{split}
\end{align}
It is clear that
$$
\mathbb{E}(\bar{Y}) = \begin{pmatrix} 1/l \\ \vdots \\ 1/l \end{pmatrix}^\top \mathbb{E}(\tilde{\mathbf{X}}^{\text{raw}}_j) = \begin{pmatrix} 1/l \\ \vdots \\ 1/l \end{pmatrix}^\top \boldsymbol{\mu}^\text{raw} = \frac{1}{l}\sum_{i = 1}^l \mu^\text{raw}_i.
$$
The expression for $S^2$ is known to be an unbiased estimator for the variance of $Y_j^{(1)}$, i.e.
$$
\mathbb{E}(S^2) = \text{Var}(Y_j^{(1)}) = \begin{pmatrix} 1/l \\ \vdots \\ 1/l \end{pmatrix}^\top \mathbf{\Sigma}^\text{raw} \begin{pmatrix} 1/l \\ \vdots \\ 1/l \end{pmatrix} = \ \frac{1}{l^2} \sum_{i = 1}^l \sum_{i' = 1}^n \sigma^\text{raw}_{i', i}.
$$
This completes the proof.

\subsection{Derivations for Section \ref{subsec:theory_sensitivity}}
\label{suppl:derivations_sensitivity}

As the exceedance of thresholds is invariate to shifting and scaling of the $Y^{(i)}_j$ we can for simplicity assume that they follow a standard normal distribution with $\mu_1 = 0, \sigma^2_1 = 1$. Now consider
$$
\hat{q}_{Y, \alpha} = \bar{Y} + z_\alpha S.
$$
Basu's theorem tells us that the sample mean and standard deviation are independent. We can thus  address their respective distributions separately. We obviously have
$$
\bar{Y} \sim \text{N}\left(0, \frac{1}{m}\right).
$$
It is moreover known that for the sample variance $S^2$ of $m$ standard normal random variables
$$
[(m - 1) \times S^2] \sim \chi^2(m - 1)
$$
holds \citep{HELM2008}. The square root of a $\chi^2$ distributed random variable can be approximated well by a normal distribution \citep[p426]{Johnson1994}. Specifically, we get
$$
\sqrt{2 \times (m - 1) \times S^2} \ \  \stackrel{\text{approx}}{\sim} \ \ \text{N}(\sqrt{2m - 2}, 1)
$$
and thus
$$
S \ \  \stackrel{\text{approx}}{\sim} \ \ \text{N}\left(1, \ \ \frac{1}{2\times(m - 1)}\right).
$$
We can now combine these two results and use the independece of sample mean and standard deviation. For the simpler threshold definition \eqref{eq:q_Y} we get
$$
\hat{q}_{Y, \alpha} \ \  \stackrel{\text{approx}}{\sim} \ \ \text{N}\left[z_\alpha, \frac{1}{m} + z_\alpha^2 \times \frac{1}{2 \times (m - 1)} \right].
$$
Equation \eqref{eq:q_Y} is then obtained by simple re-scaling and shifting.

We can now compute the sensitivity. It corresponds to the probability that $Y^{(1)}_{m + 1} > \hat{q}_{Y, \alpha}$ given exceedance $Y^{(1)}_{m + 1} > z_\alpha$ (keep in mind that $Y^{(1)}_{m + 1}$ it is assumed to follow a standard normal distribution). This conditional probability is straightforward to evaluate using the integral in equation \eqref{eq:sens}. The derivation of the specificity follows the same argument.

For the slightly more complex formulation \eqref{eq:q_Y_t} we obtain
\begin{equation}
\hat{q}_{Y, \alpha} \ \  \stackrel{\text{approx}}{\sim} \ \ \text{N}\left[t_{m \times n - 1, \alpha} \times \sqrt{1 + \frac{1}{m \times n}}, \frac{1}{m} + t_{m \times n - 1, \alpha}^2 \times \left(1 + \frac{1}{m} \right) \times \frac{1}{2 \times (m - 1)} \right] \label{eq:approx_distr_q}
\end{equation}
as the approximate distribution of the thresholds. The expressions for the sensitivity and specificity are of the same general form
\begin{align}
\text{sens}_\alpha & = \text{Pr}\left(Y_{m + 1}^{(1)} > \hat{q}_{Y, \alpha} \ \mid \ Y_{m + 1}^{(1)} > q_{Y, \alpha}\right) \approx \int_{z_\alpha}^\infty \phi(y) \times \Phi\left(\frac{y - \mathbb{E}(\hat{q}_{Y, \alpha})}{\text{sd}(\hat{q}_{Y, \alpha})}\right) \text{d}y,
\label{eq:sens_t}\\
\text{spec}_\alpha & = \text{Pr}\left(Y_{m + 1}^{(1)} < \hat{q}_\alpha \ \mid \ Y_{m + 1}^{(1)} < q_{Y, \alpha}\right) \approx \int_{-\infty}^{z_\alpha}\phi(y) \times \left\{1 - \Phi\left(\frac{y - \mathbb{E}(\hat{q}_{Y, \alpha})}{\text{sd}(\hat{q}_{Y, \alpha})}\right)\right\} \text{d}y,
\end{align}
where in order to simplify the display we did not write out the mean and variance from equation \eqref{eq:approx_distr_q}.



\newpage

\section{Supplementary figures based on French data}

\begin{figure}[h!]
\includegraphics[width=0.95\textwidth]{figure/plot_smoothing7_fr_small.pdf}
\caption{Impact of smoothing of historical data on thresholds. We applied a moving average with $l = 7$ to the historical time series prior to computing thresholds and subsequently applied them to either unsmoothed or smoothed new peak values. Results are shown for thresholds computed with a log transformation. See the legend of Figure \ref{fig:results1} for details on the plot elements.}
\label{fig:results_smoothing7}
\end{figure}

\newpage

\begin{figure}[h!]
\includegraphics[width=0.9\textwidth]{figure/mosaic_fr_fancy.pdf}
\caption{Intensity classifications obtained with different choices of $n$ and transformation function $f$, as a function of the true quantile level of a season peak (i.e., with respect to the true distribution). This corresponds to a more detailed version of Figure \ref{fig:mosaic}.}
\label{fig:mosaic_fancy}
\end{figure}

\newpage


\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{figure/mosaic_log_smoothed_fr.pdf}

\caption{Confusion matrices for intensity classifications obtained with a smoothing window of width $l = 3$ and a log transformation. Top: thresholds applied to unsmoothed new peaks; bottom: thresholds applied to smoothed new peaks. Mosaic plots show which fractions of season peaks which are truly very high, high, medium or low are classified into the four categories, see caption of Figure \ref{fig:mosaic} for details.}
\label{fig:mosaic_smoothing}
\end{center}
\end{figure}


\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{figure/mosaic_t_fr.pdf}

\caption{Confusion matrices for intensity classifications based on quantiles of a $t$ rather than normal distribution. Mosaic plots show which fractions of season peaks which are truly very high, high, medium or low are classified into the four categories, see caption of Figure \ref{fig:mosaic} for details.}
\label{fig:mosaic_t}
\end{center}
\end{figure}

%\begin{figure}[h!]
%\begin{center}
%\includegraphics[width=0.7\textwidth]{figure/mosaic_log_smoothed_fr_fancy.pdf}
%\caption{Intensity classifications obtained with a smoothing window of width $l = 3$ and a log transformation, as a function of the true quantile level of a season peak (i.e., with respect to the true distribution). This corresponds to a more detailed version of Figure \ref{fig:mosaic_smoothing}. Top: thresholds applied to unsmoothed new peaks; bottom: thresholds applied to smoothed new peaks.}
%\label{fig:mosaic_smoothing_fancy}
%\end{center}
%\end{figure}

\newpage

\begin{figure}[h!]
\begin{center}
\includegraphics[width = 0.9\textwidth]{figure/plot_trend7_fr_small.pdf}
\end{center}
\caption{Average thresholds and exceedance shares in the presence of constant annual growth (7\%) and decrease (-7\%). In each setting we computed thresholds with and without accounting for the secular trend. This parallels Figure \ref{fig:trend}.}
\label{fig:trend7}
\end{figure}

\newpage


\begin{figure}[h!]
\begin{center}
\includegraphics[width = 0.8\textwidth]{figure/plot_cost_trend_fr.pdf}
\end{center}
\vspace{-10mm}
\caption{Sensitivity, specificity and positive predictive values in three settings involving secular trends. Top row: now true secular trend. Middle row: secular trend with a yearly growth rate of 3\%. Bottom: Secular trend with a yearly growth rate of 7\%. In each plot we display the respective values as a function of $m$ and whether the secular trend was accounted for in thresholds or not (filled squares: not accounted; unfilled: accounted).}
\label{fig:cost_trend}
\end{figure}

\newpage

\section{Supplementary figures based on US data}
\label{suppl:us}

In the following, Figures \ref{fig:data}--\ref{fig:cis} are reproduced using the data from the US described in Section \ref{subsec:data}.

\begin{figure}[h]
\center
\includegraphics[width=1\textwidth]{figure/plot_data_us.pdf}
\caption{Reproduction of Figure \ref{fig:data} using US data. Re-scaled Time series of weekly weighted ILI percentages in US HHS Regions 1 and 7, 1999--2018. Off-season weeks are omitted in the plot, with grey lines delimiting the different seasons. The bottom row shows descriptive plots of the distribution of season peaks. First: Boxplots of values by rank within season. Second: Boxplot of smoothed peak values as a function of the smoothing window width $l$. Third: Normal QQ plot of untransformed peak values. Fourth: Normal QQ plot of log-transformed peak values.}
\label{fig:data_us}
\end{figure}

\newpage

\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{figure/plot_us.pdf}

\vspace{-1.5cm}

\caption{Reproduction of Figure \ref{fig:results1} using US data. Impact of the choice of $n$ and transformation function $f$. First column: simulation-based average intensity thresholds (squares) along with bands delimited by the empirical 5\% and 95\% quantiles. Analytical approximations of mean threshold values (computed from empirical means and covariance matrices) are displayed as lines. Second column: resulting average shares of season peaks classified as low, medium, high and very high intensity. Third to fifth columns: sensitivity, specificity and PPVs of the different thresholds. Simulation results are shown as squares. Where available analytical approximations are shown as lines.}
\label{fig:results1_us}
\end{figure}

\newpage

\begin{figure}
\includegraphics[width=0.95\textwidth]{figure/mosaic_us.pdf}
\caption{Reproduction of Figure \ref{fig:mosaic} using US data. Confusion matrices for intensity classifications obtained with different choices of $n$ and transformation function $f$. Mosaic plots show which fractions of season peaks which are truly very high, high, medium or low are classified into the four categories. The true class is determined with respect to the empirical quantiles of the distribution of peaks: very high (highest 2.5\% of all peaks), high (next 7.5\%), medium (next 50\%), low (lowest 40\% of all peaks).}
\label{fig:mosaic_us}
\end{figure}

\newpage

\begin{figure}[h!]
\centering
\includegraphics[width=1\textwidth]{figure/plot_smoothing3_us_small.pdf}

\caption{Reproduction of Figure \ref{fig:results_smoothing} using US data. Impact of smoothing of historical data on thresholds. We applied a moving average with $l = 3$ to the historical time series prior to computing thresholds and subsequently applied them to either unsmoothed or smoothed new peak values. Results are shown for thresholds computed with a log transformation. See the caption of Figure \ref{fig:results1} for details on the plot elements.}
\label{fig:results_smoothing_us}
\end{figure}

\newpage


\begin{figure}
\begin{center}
\includegraphics[width=0.7\textwidth]{figure/mosaic_log_smoothed_us.pdf}
\end{center}
\caption{Reproduction of Figure \ref{fig:mosaic_smoothing} using US data. Confusion matrices for intensity classifications obtained with a smoothing window of width $l = 3$ and a log transformation. Top: thresholds applied to unsmoothed new peaks; bottom: thresholds applied to smoothed new peaks. Mosaic plots show which fractions of season peaks which are truly very high, high, medium or low are classified into the four categories, see caption of Figure \ref{fig:mosaic} for details.}
\label{fig:mosaic_smoothing_us}
\end{figure}



\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.7\textwidth]{figure/mosaic_t_fr.pdf}

\caption{Reproduction of Figure \ref{fig:mosaic_t} using US data. Confusion matrices for intensity classifications based on quantiles of a $t$ rather than normal distribution. Mosaic plots show which fractions of season peaks which are truly very high, high, medium or low are classified into the four categories, see caption of Figure \ref{fig:mosaic} for details.}
\label{fig:mosaic_t_us}
\end{center}
\end{figure}

\newpage

\begin{figure}[h!]
\includegraphics[width=0.9\textwidth]{figure/mosaic_fr_fancy.pdf}
\caption{Reproduction of Figure \ref{fig:mosaic_fancy} using US data. Intensity classifications obtained with different choices of $n$ and transformation function $f$, as a function of the true quantile level of a season peak (i.e., with respect to the true distribution). This corresponds to a more detailed version of Figure \ref{fig:mosaic_us}.}
\label{fig:mosaic_fancy_us}
\end{figure}

\newpage


\begin{figure}
\begin{center}
\includegraphics[width = 0.9\textwidth]{figure/plot_trend3_us_small.pdf}
\end{center}
\caption{Reproduction of Figure \ref{fig:trend} using US data. Average thresholds and exceedance shares in the presence of constant annual growth (3\%) and decrease (-3\%). In each setting we computed thresholds with and without accounting for the secular trend.}
\label{fig:trend_us}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width = 0.9\textwidth]{figure/plot_trend7_us_small.pdf}
\end{center}
\caption{Reproduction of Figure \ref{fig:trend7} using US data. Average thresholds and exceedance shares in the presence of constant annual growth (7\%) and decrease (-7\%). In each setting we computed thresholds with and without accounting for the secular trend. This parallels Figure \ref{fig:trend}.}
\label{fig:trend7_us}
\end{figure}


\newpage

\begin{figure}
\includegraphics[width=0.93\textwidth]{figure/plot_ci_us.pdf}
\caption{Reproduction of Figure \ref{fig:cis} using US data. Average thresholds and exceedance shares when thresholds are based on confidence intervals rather than prediction intervals. See the caption and legend of Figure \ref{fig:results1} for details on the plot elements.}
\label{fig:cis_us}
\end{figure}

\newpage



\begin{figure}[h!]
\begin{center}
\includegraphics[width = 0.95\textwidth]{figure/plot_cost_trend_us.pdf}
\end{center}
\caption{Reproduction of Figure \ref{fig:cost_trend} using US data. Sensitivity, specificity and positive predictive values in three settings involving secular trends. Top row: now true secular trend. Middle row: secular trend with a yearly growth rate of 3\%. Bottom: Secular trend with a yearly growth rate of 7\%. In each plot we display the respective values as a function of $m$ and whether the secular trend was accounted for in thresholds or not (filled squares: not accounted; unfilled: accounted).}
\end{figure}

%\begin{figure}[h!]
%\begin{center}
%\includegraphics[width=0.87\textwidth]{figure/mosaic_log_smoothed_fr_fancy.pdf}
%\caption{Reproduction of Figure \ref{fig:mosaic_smoothing_us} using US data. Intensity classifications obtained with a smoothing window of width $l = 3$ and a log transformation, as a function of the true quantile level of a season peak (i.e., with respect to the true distribution). This corresponds to a more detailed version of Figure \ref{fig:mosaic_smoothing_us}. Top: thresholds applied to unsmoothed new peaks; bottom: thresholds applied to smoothed new peaks.}
%\label{fig:mosaic_smoothing_fancy_us}
%\end{center}
%\end{figure}

\end{document}